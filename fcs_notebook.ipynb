{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:37.546475Z",
     "start_time": "2019-05-23T14:52:37.134676Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:37.550482Z",
     "start_time": "2019-05-23T14:52:37.548081Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.facecolor'] = 'w'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual FCS, using MD Simulation Data\n",
    "\n",
    "This goal of this notebook is to generate a simulated FCS measurement using data from a GROMACS simulation.\n",
    "\n",
    "The setup of the virtual system is a membrane with a circularly symmetric incident beam.\n",
    "\n",
    "Data is read in from an .xtc or .trr file, using a .gro file to define the system topology. Data frames from the input file are iterated through, and for each frame, a detected intensity from each lipid is calculated. The intensity trace and autocorrelation functions for each lipid, and for the total at each frame, are plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \u2612 Why don't autocorrelation curves look right? Flatness in middle is unexpected\n",
    "- Done. See below\n",
    " \n",
    "#### \u2612 Generate and plot autocorrelation curve prediction. I'm still not understanding something about autocorrelation curves - I don't see where the sigmoid shape comes from.\n",
    " - Done. Curves need to be plotted on a semilog scale in order to see the expected sigmoid shape.\n",
    " \n",
    "#### \u2612 Try autocorrelating the data myself instead of using acorr. Maybe something weird is happening with the way matplotlib does autocorrelation that isn't desirable.\n",
    " - Done. The manually autocorrelated data is identical to calling plt.acorr with normed=True\n",
    " \n",
    "#### \u2612 Parallelize data analysis\n",
    " - Tried it. The analysis is quite computationally cheap compared to the extra overhead from spawning new threads -- not worth the time saved by running the analysis in parallel.\n",
    " \n",
    "#### \u2612 Handle breaking data up into bins better -- currently ignores the remainder of lipids that don't fall into a bin. (I.e. 11 lipids, bin size 3, the remaining 2 that don't evenly fall into bins are discarded.)\n",
    " - Done. The leftover lipids are put into their own bin, which may be smaller than the BIN_SIZE.\n",
    " \n",
    "#### \u2612 How do PBC vs unwrapped affect prediction of diffusion constant?\n",
    " - Done. Don't appear to significant affect results.\n",
    " \n",
    "#### \u2610 Curve fit method seems to work better for small spot sizes, but 0.5-crossing method seems better for larger spots? Why?\n",
    "\n",
    "#### \u2610 Come up with a way to score the different tests in the Excel spreadsheet. Factor in how close the average is, and how big the error is.\n",
    "     \n",
    "[Checkbox symbols]:<> (\u2612 \u2610)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's recommended to 'unwrap' simulation data to remove potential artifacts from periodic boundary conditions in the simulation.\n",
    "\n",
    "### Method 1 (Better for big systems)\n",
    "The *best* way of doing this requires a trajectory file (i.e. `.xtc`, `.trr`), a `.gro`, and a `.tpr`. By doing this, the input data filesizes can be significantly reduced off the bat by selecting only the lipid groups to keep in the new trajectory file. This can be accomplished by running the following.\n",
    "\n",
    "When prompted to select a group, select only the group of lipids.\n",
    "\n",
    "`$>gmx trjconv -f <trajectory file> -s <.gro file> -o <output trajectory file> -pbc nojump`\n",
    "\n",
    "`$>gmx trjconv -f <.gro file> -s <.tpr file> -o <output .gro file> -pbc nojump`\n",
    "\n",
    "### Method 2\n",
    "This can also be done in one step, with only a trajectory file and a `.gro` file by selecting the group of ALL atoms when prompted.\n",
    "\n",
    "`$>gmx trjconv -f <trajectory file> -s <.gro file> -o <output trajectory file> -pbc nojump`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Notes on input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:37.562318Z",
     "start_time": "2019-05-23T14:52:37.551845Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trajectory_file = \"40nm/run_nojump.xtc\"\n",
    "trr_file = \"40nm/run.trr\"\n",
    "topology_file = \"40nm/system_nojump.gro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.001661Z",
     "start_time": "2019-05-23T14:52:37.563670Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mdtraj as md\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit, fsolve, root\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from threading import Lock # For print statements\n",
    "\n",
    "import trajectory\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.005240Z",
     "start_time": "2019-05-23T14:52:38.003008Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.034419Z",
     "start_time": "2019-05-23T14:52:38.006458Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_spots(x):\n",
    "    return x\n",
    "\n",
    "interact(generate_spots, x=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Detection area parameters\n",
    "\n",
    "`spot_radius` is used to determine whether a particle is within the detection area. Currently, this is unused, as cutoff is determined by the sigma of the beam Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.043359Z",
     "start_time": "2019-05-23T14:52:38.035555Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Radius of detection area (in nanometers)\n",
    "spot_radius = 10 # Currently unused\n",
    "\n",
    "# Coordinates of detection area center (in nanometers)\n",
    "spotX = 0.0\n",
    "spotY = 0.0\n",
    "spotZ = 10.0 # This isn't used, since we're looking at 2D membranes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian parameters\n",
    "\n",
    "Note that what's being set here is the std. deviation `w_xy` of the Gaussian - spot size is probably more accurately represented by the FWHM, or `w_xy * 2.3548`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.058938Z",
     "start_time": "2019-05-23T14:52:38.044575Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#   Radial and axial std. dev.s of the Gaussian beam profile\n",
    "FWHM = .05\n",
    "w_xy = FWHM / (2 * np.sqrt(2 * np.log(2)))\n",
    "\n",
    "w_z = 2 # Unused\n",
    "k = w_z/w_xy # Unused, just considering a 2-D membrane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.073344Z",
     "start_time": "2019-05-23T14:52:38.060170Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various simulation parameters\n",
    "\n",
    "`STEP` is the stride used when iterating through data frames. Data frames are taken every timestep.\n",
    "\n",
    "`INTENSITY` is a scaling constant used to determine the maximum intensity of fluorescence.\n",
    "\n",
    "`SAMPLING_RATIO` defines the percentage of particles to be 'tagged'. Untagged particles are discarded at the beginning of the simulation.\n",
    "\n",
    "`CUTOFF` defines a cutoff for the beam profile. This may be useful to avoid artifacts from periodic boundary conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.087548Z",
     "start_time": "2019-05-23T14:52:38.074746Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step size for iterating through data frames\n",
    "STEP = 1\n",
    "\n",
    "# Scaling constant for the intensity of a fluorescing particle\n",
    "INTENSITY = 1\n",
    "\n",
    "# Percentage of tagged particles\n",
    "SAMPLING_RATIO = .01\n",
    "\n",
    "# How many sigmas out from the beam center to truncate the beam's Gaussian profile at\n",
    "CUTOFF = 2.5\n",
    "\n",
    "# How many lipids to bin into a single trajectory\n",
    "BIN_SIZE = 1\n",
    "\n",
    "# How many random spots to use\n",
    "N_SPOTS = 8\n",
    "\n",
    "# Radius within which to randomly place spots\n",
    "SPOT_RANGE = 1000\n",
    "\n",
    "# Whether or not to filter trajectories based on start/end position\n",
    "FILTERING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diffusion parameters\n",
    "\n",
    "`D` is the diffusion constant for POPC, ~~using data from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1303347/~~ using values from Andrew's STRD paper$^{[2]}$, in units of nm^2/ns.\n",
    "\n",
    "`tauD` is the expected diffusion time, or the time of the half-max for the autocorrelation curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.100983Z",
     "start_time": "2019-05-23T14:52:38.088883Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#D = .01 From NIH paper\n",
    "D = .0245\n",
    "D = 4\n",
    "tauD = FWHM**2 / (4*D) \n",
    "print(\"Expected diffusion time is %.2f nanoseconds\" % tauD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Expected diffusion times for various beam waists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.117476Z",
     "start_time": "2019-05-23T14:52:38.102445Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beam_waists = np.arange(10,100,5) # In nanometers\n",
    "print(\"beam FWHM (nm)\".ljust(16) + \"|\" + \"tau_D (ns)\".rjust(15))\n",
    "print(\"-\"*27)\n",
    "for v in beam_waists:\n",
    "    print(str(v).ljust(16) + \"|\" + str(v ** 2 / (4*D)).rjust(15) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `check_in_detection_volume`\n",
    "Function to check if a given lipid is within the detection volume.\n",
    "\n",
    "Right now, we're interested in contributions from all lipids regardless of position, so the `return True` short circuits it. \n",
    "\n",
    "Furthermore, this is somewhat deprecated by implementing the Gaussian `CUTOFF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.129582Z",
     "start_time": "2019-05-23T14:52:38.118787Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_in_detection_volume(t, frame_index, residue):\n",
    "\n",
    "    # For now, pay attention to all atoms, regardless of whether or not they're\n",
    "    #   in the detection volume. \n",
    "    # Keep this function as a placeholder, in case this changes.\n",
    "    return True\n",
    "\n",
    "    x, y, z = t.xyz[frame_index, residue._atoms[0].index]\n",
    "\n",
    "    # Get magnitude of distance to the spot center\n",
    "    distance = (x - spotX)**2 + (y - spotY)**2\n",
    "\n",
    "    # Check if the distance is within the spot radius\n",
    "    in_detection_area = distance <= spot_radius**2\n",
    "\n",
    "    return in_detection_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `generate_detection`\n",
    "Defines what happens when a detection is generated from a lipid. Right now, `INTENSITY` is weighted by a 2-D Gaussian determined by the cell's position:\n",
    "\n",
    "$I = I_0 \\exp{ \\left( - \\frac{(x-x_0)^2 + (y-y_0)^2 }{2 \\sigma^2} \\right) }$\n",
    "\n",
    "#### Inputs\n",
    "- `t`: mdtraj.Trajectory object\n",
    "- `frame_index`: index of frame to analyze in trajectory  \n",
    "- `atom`: mdtraj.Atom object to be used for detection\n",
    "\n",
    "#### Returns\n",
    "- Intensity contribution from `atom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.150040Z",
     "start_time": "2019-05-23T14:52:38.130850Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_detection(t, frame_index, atom, spotcenter):\n",
    "    \n",
    "#     print(np.shape(t.xyz))\n",
    "    _x = spotcenter[0]\n",
    "    _y = spotcenter[1]\n",
    "\n",
    "    # Get coordinates of residue (more correctly, of the P atom)\n",
    "    #x, y, z = t.xyz[frame_index, residue._atoms[0].index]\n",
    "    try:\n",
    "        x, y, z = t.xyz[frame_index, atom.index]\n",
    "    except IndexError:\n",
    "        raise IndexError(\"Indexing error. Attempting to use frame index %d and atom index %d. Shape is %s\" % (frame_index, atom.index, t.xyz.shape))\n",
    "    \n",
    "    # Get magnitude of distance to the spot center\n",
    "    distance = (x - _x)**2 + (y - _y)**2\n",
    "    \n",
    "    # Truncate at 2 sigma\n",
    "    if distance > CUTOFF**2 * w_xy**2:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate contribution to intensity from an atom, based on the Gaussian\n",
    "    #   profile of the incident beam and the particle's position.\n",
    "    intensity = \\\n",
    "        INTENSITY * np.exp(\n",
    "        -( distance ) # + ((z - spotZ)/k)**2)\n",
    "        / (2 * w_xy**2) )\n",
    "\n",
    "    return intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `analyze_frame`\n",
    "\n",
    "Analyzes the positions of atoms in a given frame, and updates the `detections` list with each atom's position.\n",
    "\n",
    "#### Inputs\n",
    "- `t`: mdtraj.Trajectory object\n",
    "- `frame_index`: index of frame to analyze in trajectory  \n",
    "- `detections`: shared list of all detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.166014Z",
     "start_time": "2019-05-23T14:52:38.151370Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_frame(t, frame_index, detections, spot_number=0, spotcenter=(spotX, spotY)): \n",
    "    \n",
    "    print(\"\\rProcessing frame %d out of %d     \" % (frame_index/STEP, len(t)/STEP), end=\"\\r\")\n",
    "\n",
    "    # Iterate through each atom remaining in the topology\n",
    "    for atom in t.topology.atoms:\n",
    "\n",
    "        # Do analysis if atom is in detection volume\n",
    "        if not check_in_detection_volume(t, frame_index, atom):\n",
    "            print(\"Not in detection volume, skipping.\")\n",
    "            continue\n",
    "\n",
    "        detected = generate_detection(t, frame_index, atom, spotcenter)\n",
    "        \n",
    "        detections[spot_number][atom.index][int(frame_index/STEP)] += detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.184491Z",
     "start_time": "2019-05-23T14:52:38.167291Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_frame_mp(t, frame_index, detections, spotcenter=(spotX, spotY)): \n",
    "    \n",
    "#     print(\"\\rProcessing frame %d out of %d     \" % (frame_index/STEP, len(t)/STEP), end=\"\\r\")\n",
    "\n",
    "    # Iterate through each atom remaining in the topology\n",
    "    for atom in t.topology.atoms:\n",
    "\n",
    "        # Do analysis if atom is in detection volume\n",
    "        if not check_in_detection_volume(t, frame_index, atom):\n",
    "#             print(\"Not in detection volume, skipping.\")\n",
    "            continue\n",
    "\n",
    "        detected = generate_detection(t, frame_index, atom, spotcenter)\n",
    "        \n",
    "#         print(atom.index)\n",
    "        detections[atom.index][int(frame_index/STEP)] += detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autocorr_model`\n",
    "\n",
    "Models the expected autocorrelation curve at a given time, for a given diffusion constant.\n",
    "\n",
    "#### Inputs\n",
    "- `t`: Time to calculate autocorrelation at\n",
    "- `D`: Diffusion constant\n",
    "\n",
    "#### Returns\n",
    "- Value of autocorrelation curve at time `t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.200818Z",
     "start_time": "2019-05-23T14:52:38.185736Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def autocorr_model(t, D):\n",
    "    tauD = FWHM**2 / (4*D)\n",
    "    \n",
    "    return (1 + t / tauD)**(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autocorrelate`\n",
    "Computes the autocorrelation curve for a set of data.\n",
    "\n",
    "#### Inputs\n",
    "- `_data`: 1-D array of data\n",
    "- `normed`: Boolean, determines whether to normalize data to unity. Default - `True`\n",
    "\n",
    "#### Returns\n",
    "- `autocorrelated`: Autocorrelation for input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:57:42.531374Z",
     "start_time": "2019-05-23T14:57:42.504715Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def autocorrelate(_raw, normed=True, reduction=1, step=1):\n",
    "    \n",
    "    _data = _raw\n",
    "    if normed:\n",
    "        _data = _raw/np.linalg.norm(_raw)\n",
    "        \n",
    "    autocorrelated = np.correlate(_data, _data, mode='full')\n",
    "    autocorrelated = autocorrelated[(autocorrelated.size-1)//2 :]\n",
    "\n",
    "#     print(\"Length of data is %d\" % len(_data))\n",
    "#     max_lag = len(_data)//reduction\n",
    "#     print(\"Max lag is %d\" % max_lag)\n",
    "# #     time_lags = np.arange(max_lag)\n",
    "#     G = np.zeros(max_lag)\n",
    "#     I = np.mean(_data)\n",
    "#     stdI = np.std(_data)\n",
    "#     M = len(_data)\n",
    "#     tau = 1\n",
    "#     for dt in range(max_lag):\n",
    "        \n",
    "#         m = dt\n",
    "        \n",
    "        \n",
    "#         G[dt] = np.sum( [ \\\n",
    "#                 (_data[tau*i] - I) * \\\n",
    "#                 (_data[tau*i + m*tau] - I) \\\n",
    "#             for i in range(0,M-m,step)])\n",
    "        \n",
    "#         G[dt] = G[dt] / (stdI**2 * (M-m))\n",
    "        \n",
    "#         if dt % 100 == 0:\n",
    "#             print(\"Calculated %d\" % dt, end=\"\\r\")\n",
    "#     autocorrelated = G\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return autocorrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gauss2d`\n",
    "\n",
    "Computes the value of a 2-D Gaussian.\n",
    "\n",
    "#### Inputs\n",
    "- `xy`: list of (x,y) tuples\n",
    "- `spot`: (x,y) tuple of coordinates of center of detection area\n",
    "- `sigma`: Standard deviation of Gaussian\n",
    "- `cutoff`: Number of standard deviations after which to truncate Gaussian and return 0. Default - `CUTOFF`\n",
    "\n",
    "#### Returns\n",
    "- `z`: A flat list of the value of the Gaussian at each input point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.256258Z",
     "start_time": "2019-05-23T14:52:38.244718Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " def gauss2d(xy, spot, sigma, cutoff=CUTOFF):\n",
    "    x, y = xy\n",
    "    x0, y0 = spot\n",
    "    distance = (x - x0)**2 + (y - y0)**2\n",
    "    \n",
    "    # Set values outside the cutoff to 0\n",
    "    s2 = (cutoff * sigma)**2 # Define this so I don't have to keep recomputing it in the list comprehension\n",
    "    distance = np.array([d if d < s2 else 1e10 for d in distance])\n",
    "    \n",
    "    z = np.exp(-(distance)/(2 * sigma**2))\n",
    "    \n",
    "    # Returns an array of the Z values\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_crossings`\n",
    "Gets the 0-crossing of a set of normalized data. This will only find one 0-crossing and can be quite sensitive to the initial guess provided to the solver.\n",
    "\n",
    "#### Inputs\n",
    "- `x`: List of x values\n",
    "- `data`: Normalized set of data\n",
    "- `guess`: Initial guess to solver (not used any more, went for a simpler approach)\n",
    "\n",
    "#### Returns\n",
    "- `crossing`: x coordinate of 0 crossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.294678Z",
     "start_time": "2019-05-23T14:52:38.257642Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_crossing(x, data, guess):\n",
    "    \n",
    "    if np.isnan(data).all():\n",
    "        raise ValueError(\"All NaN, no intensity contributions\")\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    crossing = 0\n",
    "    for t in range(len(data)):\n",
    "        if data[t] > 0.5:\n",
    "            crossing+=1 # Go to the next point if you haven't crossed yet\n",
    "            \n",
    "        else:\n",
    "            # Do a little linear interpolation between the crossed point and the previous\n",
    "            if t == 0:\n",
    "                start = 1\n",
    "            else:\n",
    "                start = data[t-1]\n",
    "                \n",
    "            m = start-data[t]\n",
    "            crossing = (start - 0.5) / m\n",
    "            \n",
    "            return t-1 + crossing\n",
    "        \n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     interpolated = interp1d(x, data - .5, fill_value='extrapolate')\n",
    "# #     interpolated = np.interp(x, data - .5, left=1, right=0)\n",
    "    \n",
    "    \n",
    "#     try:\n",
    "#         crossing = fsolve(interpolated, 0)# tauD)\n",
    "#     except ValueError:\n",
    "#         print(\"**** BAD GUESS *****\")\n",
    "#         print(\"Skipping this spot...\")\n",
    "#         raise ValueError(\"Bad guess.\")\n",
    "#         return False\n",
    "        \n",
    "    return crossing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `analyze_spot_mp`\n",
    "\n",
    "Function to analyze detections from a given spot. Suitable for multiprocessed applications.\n",
    "\n",
    "#### Inputs\n",
    "- `t`: Trajectory (or FakeTrajectory) object\n",
    "- `spot`: (x,y) tuple of center of spot to use\n",
    "- `detections`: Queue object to store detections in\n",
    "- `spot_num`: Index of spot.\n",
    "\n",
    "#### Returns\n",
    "- None. However, will add a tuple of (spot_num, \\_detections) to the queue, where detections is an np array of shape (n_residues, number of timesteps) storing the detections for each residue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.308145Z",
     "start_time": "2019-05-23T14:52:38.296075Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_spot(t, spot, detections, spot_num):\n",
    "    for frame_index in range(0, len(t), STEP):\n",
    "        analyze_frame(t, frame_index, detections, spot_num, spot)\n",
    "\n",
    "def analyze_spot_mp(t, spot, detections, spot_num):\n",
    "    _detections = np.full(shape=(t.topology.n_residues, int(np.ceil(len(t)/STEP))), fill_value=0.0) \n",
    "    for frame_index in range(0, len(t), STEP):\n",
    "        analyze_frame_mp(t, frame_index, _detections, spot)\n",
    "    detections.put((spot_num, _detections))\n",
    "    \n",
    "    print(\"{} particles contribute to spot {} detections\".format(len(_detections), spot_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `calibrate`\n",
    "Function to calibrate results of diffusion constant calculation.\n",
    "\n",
    "#### Inputs\n",
    "- `dtype`: CROSSING or CURVE_FIT or ALL_AVG, constants which map to 0 or 1. Determine the calibration to use.\n",
    "- `D`: Diffusion constant or list of diffusion constants to calibrate\n",
    "\n",
    "#### Returns\n",
    "- `D`: Corrected diffusion constant, or list of diffusion constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.325148Z",
     "start_time": "2019-05-23T14:52:38.309459Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CROSSING = 0\n",
    "CURVE_FIT = 1\n",
    "ALL_AVG = 2\n",
    "\n",
    "def calibrate(dtype, D):\n",
    "    D = np.array(D)\n",
    "    return[\n",
    "        D - .0151*FWHM - 4.3649,\n",
    "        D - .0671*FWHM + .0454,\n",
    "        D - .1023*FWHM - 4.6588\n",
    "    ][dtype]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load trajectory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Import FCS data from .xtc file. Also specify a .gro file for the system topology.\n",
    "\n",
    "**NB:** A .trr file can be used for better resolution, since an .xtc typically uses some compression. However, a .trr is also much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.343778Z",
     "start_time": "2019-05-23T14:52:38.326456Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# t = md.load(trajectory_file, top=topology_file)\n",
    "#t = md.load_trr(trr_file, top=topology_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Calculate timestep for data analysis, given the simulation data timestep and current stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.358755Z",
     "start_time": "2019-05-23T14:52:38.345076Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"Timestep for data analysis is %.2f picoseconds (%.2f nanoseconds)\" % (t.timestep * STEP, t.timestep * STEP / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.370471Z",
     "start_time": "2019-05-23T14:52:38.360216Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"Starting with %d atoms\" % t.topology.n_atoms)\n",
    "\n",
    "# phosphorous_atoms = [a.index for a in t.topology.atoms if a.element.symbol == 'P']\n",
    "# t.atom_slice(phosphorous_atoms, inplace=True)\n",
    "\n",
    "# print(\"Reduced to %d phosphorous atoms\" % t.topology.n_atoms)\n",
    "\n",
    "# # Reduce to the sampling ratio * number of phosphorous atoms\n",
    "# num_sampled = int(t.topology.n_atoms * SAMPLING_RATIO)\n",
    "\n",
    "# # Randomly select the sampled atoms\n",
    "# sampled = np.random.choice([a.index for a in t.topology.atoms], num_sampled, replace=False)\n",
    "# t.atom_slice(sampled, inplace=True)\n",
    "\n",
    "# print(\"Reduced to %d \\\"tagged\\\" phosphorous atoms\" % t.topology.n_atoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Reduce atom selection to only phosphorous atoms\n",
    "\n",
    "This is a bit of a simplification, but significantly reduces the amount of atoms to iterate over if we're only considering the phosphorous at the center of the phosphate group. Error from this would be on the order of the bond lengths, so roughly 1.5 angstrom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.500044Z",
     "start_time": "2019-05-23T14:52:38.372313Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle, trajectory\n",
    "# t = pickle.load(open('D4.pkl', 'rb'))\n",
    "# t = pickle.load(open('../../windrive/linux/D15.pkl', 'rb'))\n",
    "# t = pickle.load(open('/home/jd/phd/virtual_fcs/D15.pkl', 'rb'))\n",
    "t = pickle.load(open('Lo_2.pkl', 'rb'))\n",
    "# print(\"Shape is %s, reducing by %d%%.\" % (np.shape(t.xyz), 100-SAMPLING_RATIO*100))\n",
    "# t.remove(range(50000,len(t)))\n",
    "# t.reduce(SAMPLING_RATIO)\n",
    "print(np.shape(t.xyz))\n",
    "\n",
    "trajectories = []\n",
    "print(int(t.topology.n_residues / BIN_SIZE))\n",
    "\n",
    "# BIN_SIZE is used a little differently with this technique -- applies to binning trajectories in the experiment itself, rather than binning experimental data during analysis.\n",
    "# import copy\n",
    "for traj in range(int(t.topology.n_residues / BIN_SIZE)):\n",
    "    _t = trajectory.FakeTrajectory()\n",
    "    _t.initialize(t.xyz[:,BIN_SIZE*traj:BIN_SIZE*(traj+1)], BIN_SIZE)\n",
    "#     _t = copy.deepcopy(t)\n",
    "#     _t.xyz = t.xyz[:,BIN_SIZE*traj:BIN_SIZE*(traj+1)]\n",
    "#     _t.topology.n_residues = BIN_SIZE\n",
    "#     _t.topology.atoms = t.topology.atoms[BIN_SIZE*traj:BIN_SIZE*(traj+1)]\n",
    "#     _t.reindex()\n",
    "    trajectories.append(_t)\n",
    "# N_SPOTS = len(trajectories)\n",
    "# print(N_SPOTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of lists to store detected intensity at each timestep for each lipid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.504730Z",
     "start_time": "2019-05-23T14:52:38.501527Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "detections = np.full(shape=(N_SPOTS, t.topology.n_residues, int(np.ceil(len(t)/STEP))), fill_value=0.0)\n",
    "# detections = np.full(shape=(len(trajectories), BIN_SIZE, int(np.ceil(len(t)/STEP))), fill_value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Iterate through each frame of data in the trajectory file, and generate a detected intensity from each lipid (represented by its head group P atom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.519704Z",
     "start_time": "2019-05-23T14:52:38.506103Z"
    }
   },
   "outputs": [],
   "source": [
    "# _oldspots = spot_centers\n",
    "# lo = _oldspots\n",
    "# ld = spot_centers\n",
    "\n",
    "# spot_centers = lo\n",
    "# spot_centers = np.array([ [np.random.uniform(1.,2.), np.random.uniform(1.5,3.)] for x in range(N_SPOTS)])\n",
    "# spot_centers = np.array([ [np.random.uniform(-SPOT_RANGE,SPOT_RANGE), np.random.uniform(-SPOT_RANGE,SPOT_RANGE)] for x in range(N_SPOTS)])\n",
    "\n",
    "spot_centers = [[1.57666814, 2.27445561],\n",
    " [1.45451181, 2.28472249],\n",
    " [1.5820474,  2.315859  ],\n",
    " [1.60105771, 2.23473633],\n",
    " [1.49375276, 2.15073091],\n",
    " [1.61871955, 2.36478765],\n",
    " [1.57338603, 2.28992898],\n",
    " [1.46471912, 2.37245311]]\n",
    "# [[1.96604812 1.91627846]\n",
    "#  [1.76389349 2.97785923]\n",
    "#  [1.15984199 2.43058837]\n",
    "#  [1.22271718 1.86908229]\n",
    "#  [1.28333494 2.76935033]\n",
    "#  [1.01740676 2.50635394]\n",
    "#  [1.3402123  1.93760266]\n",
    "#  [1.03711774 1.88395322]]\n",
    "\n",
    "print(spot_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out any trajectories that are already in the spot area at t=0 or t=t_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.542036Z",
     "start_time": "2019-05-23T14:52:38.521048Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over each single-particle trajectory\n",
    "start_filtered = 0\n",
    "end_filtered = 0\n",
    "if FILTERING:\n",
    "    spotX = 0\n",
    "    spotY = 0\n",
    "    for traj in trajectories:\n",
    "        \n",
    "        start = traj.xyz[0,0,:2]\n",
    "        end = traj.xyz[-1,0,:2]\n",
    "\n",
    "\n",
    "        start_distance = (start[0] - spotX)**2 + (start[1] - spotY)**2\n",
    "        end_distance = (end[0] - spotX)**2 + (end[1] - spotY)**2\n",
    "\n",
    "#         if start_distance < (CUTOFF * w_xy)**2:\n",
    "#             traj.xyz[:,0,:2] = [3000,3000]\n",
    "#             start_filtered += 1\n",
    "        if end_distance > (CUTOFF * w_xy)**2:\n",
    "            traj.xyz[:,0,:2] = [3000,3000]\n",
    "            end_filtered += 1\n",
    "            \n",
    "print(\"%d particles filtered for start location\" % start_filtered)\n",
    "print(\"%d particles filtered for end location\" % end_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Legacy threaded implementation\n",
    "\n",
    "Much slower than the process equivalent code below. Keeping it *just in case*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This threaded code works on having each analyze_spot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:38.554324Z",
     "start_time": "2019-05-23T14:52:38.543431Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spot_centers = np.random.uniform(-SPOT_RANGE, SPOT_RANGE, size=(N_SPOTS,2))\n",
    "\n",
    "# def analyze_spot(t, spot, detections, spot_num):\n",
    "#     for frame_index in range(0, len(t), STEP):\n",
    "#         analyze_frame(t, frame_index, detections, spot_num, spot)\n",
    "\n",
    "# pool = ThreadPool()\n",
    "\n",
    "# print(\"Performing analysis for %d randomly assigned spots, parallelized over %d cores.\" % (N_SPOTS, multiprocessing.cpu_count()))\n",
    "\n",
    "# for i in range(N_SPOTS):\n",
    "#     pool.apply_async(analyze_spot, args=(t, spot_centers[i], detections, i))\n",
    "    \n",
    "# pool.close()\n",
    "# pool.join()\n",
    "\n",
    "# print(\"\\n\\n\", end='', flush=True) # Flush stdout so the printed output doesn't spread itself into the next few output cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate over a number of random spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:39.112813Z",
     "start_time": "2019-05-23T14:52:38.555741Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Performing analysis for %d randomly assigned spots, parallelized over %d cores.\" % (N_SPOTS, multiprocessing.cpu_count()))\n",
    "\n",
    "# Set up a pool to draw workers from.\n",
    "pool = multiprocessing.Pool()\n",
    "\n",
    "# Need to use a Manager to share the Queue across multiple workers in different processes.\n",
    "#   Using a Queue allows each process to access a shared object. Typically processes cannot do this\n",
    "#   but threads can. However, implementing this parallelization in processes yields a ~6x speedup\n",
    "#   over the threaded equivalent.\n",
    "m = multiprocessing.Manager()\n",
    "detections_Q = m.Queue(N_SPOTS)\n",
    "\n",
    "\n",
    "# Make a list to hold AsyncResult objects. This is useful for debugging - call job.get() for an element to see\n",
    "#    any exceptions that were raised. Otherwise, they'll fail silently.\n",
    "jobs = [] \n",
    "for i in range(N_SPOTS):\n",
    "    jobs += [pool.apply_async(analyze_spot_mp, args=(t, spot_centers[i], detections_Q, i))]\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\n",
    "# Process the queued data, and add it to the detections list.\n",
    "print(\"Processing queued data\")\n",
    "\n",
    "# detections = [x for x in iter(detections_Q.get, None)] # This should work, but throws a ValueError. Whatever.\n",
    "for i in range(N_SPOTS):\n",
    "    if detections_Q.empty():\n",
    "        print(\"Queue prematurely empty..\")\n",
    "        break\n",
    "    _spot, _detection = detections_Q.get()\n",
    "    detections[_spot] = _detection\n",
    "#     print(\"{} detections for spot {}\".format(len(detections[_spot]), _spot))\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:39.116792Z",
     "start_time": "2019-05-23T14:52:39.114561Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# detections[1].shape\n",
    "# np.sum(detections[3], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Iterate over entire dataset, vs over multiple random spots"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T01:06:56.952338Z",
     "start_time": "2018-04-01T01:06:56.046721Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "print(\"Performing analysis for %d sets of trajectories, parallelized over %d cores.\" % (len(trajectories), multiprocessing.cpu_count()))\n",
    "\n",
    "# Set up a pool to draw workers from.\n",
    "pool = multiprocessing.Pool()\n",
    "\n",
    "# Need to use a Manager to share the Queue across multiple workers in different processes.\n",
    "#   Using a Queue allows each process to access a shared object. Typically processes cannot do this\n",
    "#   but threads can. However, implementing this parallelization in processes yields a ~6x speedup\n",
    "#   over the threaded equivalent.\n",
    "m = multiprocessing.Manager()\n",
    "detections_Q = m.Queue(len(trajectories))\n",
    "\n",
    "\n",
    "# Make a list to hold AsyncResult objects. This is useful for debugging - call job.get() for an element to see\n",
    "#    any exceptions that were raised. Otherwise, they'll fail silently.\n",
    "jobs = []\n",
    "    \n",
    "# print(np.shape(trajectories[0].xyz))\n",
    "# analyze_spot_mp(trajectories[0], spot_center[0], detections_Q, 0)\n",
    "# Instead of iterating over each spot, iterate over slices of the trajectory\n",
    "for i in range(len(trajectories)):\n",
    "    jobs += [pool.apply_async(analyze_spot_mp, args=(trajectories[i], [0,0], detections_Q, i%BIN_SIZE))]\n",
    "# print([job.get() for job in jobs])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T01:07:28.253805Z",
     "start_time": "2018-04-01T01:06:56.955806Z"
    },
    "hidden": true
   },
   "source": [
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T20:34:46.095933Z",
     "start_time": "2018-04-02T20:34:46.086986Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "# Process the queued data, and add it to the detections list.\n",
    "print(\"Processing queued data\")\n",
    "\n",
    "# detections = [x for x in iter(detections_Q.get, None)] # This should work, but throws a ValueError. Whatever.\n",
    "for i in range(len(trajectories)):\n",
    "#     print(\"Analyzing queue entry %d\" % i)\n",
    "    if detections_Q.empty():\n",
    "        print(\"Queue prematurely empty..\")\n",
    "        break\n",
    "    _spot, _detection = detections_Q.get()\n",
    "    detections[_spot] = _detection\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:39.128661Z",
     "start_time": "2019-05-23T14:52:39.118030Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detections.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin lipids in data\n",
    "\n",
    "`binned_tots[bin][timestep]` is the total intensity for a certain bin at a certain timestep\n",
    "\n",
    "`binned_avgs[spot][bin]` is the average intensity for a bin, over the whole time\n",
    "\n",
    "`binned_dI[spot][bin][timestep]` is the difference of the total intensity from the average at a given timestep\n",
    "\n",
    "`binned_tot[spot][timestep]` is the average dI from all bins at a certain timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:39.286207Z",
     "start_time": "2019-05-23T14:52:39.129884Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#n_bins = int(np.ceil(trajectories[0].topology.n_residues/BIN_SIZE))\n",
    "n_bins = int(np.ceil(t.topology.n_residues/BIN_SIZE))\n",
    "\n",
    "print(\"{} residues in this trajectory\".format(trajectories[0].topology.n_residues))\n",
    "\n",
    "# print(\"Attempting to bin %d residues into %d bins of size %d.\" % (t.topology.n_residues, n_bins, BIN_SIZE))\n",
    "\n",
    "if not t.topology.n_residues%BIN_SIZE == 0:\n",
    "    print(\"Number of residues is not evenly divisible by bin size. Desired size is %d, one bin will contain %d.\" % (BIN_SIZE, t.topology.n_residues%BIN_SIZE) )\n",
    "\n",
    "binned_tots = np.zeros(shape=(N_SPOTS, n_bins, len(t)))\n",
    "binned_avgs = np.zeros(shape=(N_SPOTS, n_bins, len(t)))\n",
    "binned_dI = np.zeros(shape=(N_SPOTS, n_bins, len(t)))\n",
    "binned_tot = np.zeros(shape=(N_SPOTS, len(t)))\n",
    "    \n",
    "    \n",
    "# With the method of spots, this doesn't work as it should <- What does this mean?\n",
    "num_detections = 0\n",
    "for spot_num in range(N_SPOTS):\n",
    "#     print(\"Sorting data into %d groups\" % n_bins)\n",
    "\n",
    "    binned =  [ [] for x in range(t.topology.n_residues//BIN_SIZE) ]\n",
    "\n",
    "\n",
    "    # TODO: May want to use np.random.choice to randomly select the binned lipids, though the choice of lipids to sample is already random\n",
    "    # For each group...\n",
    "    for g in range(n_bins):\n",
    "\n",
    "        # Pick the slice of detections that are relevant to this bin\n",
    "        _detections = [x for x in detections[spot_num][g*BIN_SIZE:BIN_SIZE*(g+1)]]\n",
    "        \n",
    "#         print(np.count_nonzero(_detections))\n",
    "        # If no detections, remove this from data\n",
    "        if np.count_nonzero == 0:\n",
    "            continue\n",
    "#         print(len(_detections))\n",
    "\n",
    "        # Average and total intensity at each timestep\n",
    "        avg_I = np.mean(_detections)\n",
    "        tot_I = np.sum(_detections, axis=0)\n",
    "        \n",
    "        num_detections += np.count_nonzero(_detections)\n",
    "    \n",
    "        print(\"Bin/Spot: {}/{} | Detections: {}\".format(g, spot_num, np.count_nonzero(_detections)))\n",
    "\n",
    "        # The delta is the difference between the sum of the detections at a timestep\n",
    "        # and the average across all timesteps.\n",
    "        delta_I = [tot_I[x] - avg_I for x in range(len(tot_I))]\n",
    "\n",
    "\n",
    "        binned_tots[spot_num][g] = tot_I\n",
    "        binned_avgs[spot_num][g] = avg_I\n",
    "        binned_dI[spot_num][g] = delta_I\n",
    "\n",
    "    # binned_tot = np.sum(binned_tots, axis=0) - np.mean(binned_tots)\n",
    "    \n",
    "    \n",
    "    binned_tot[spot_num] = np.mean(binned_dI[spot_num], axis=0) # Average dI across bins for each spot\n",
    "print(\"Detected %d intensity contributions\" % num_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute average numbers of particles contributing to the intensity. (I.e., that have a nonzero intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:39.302609Z",
     "start_time": "2019-05-23T14:52:39.287695Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_nonzero = 0\n",
    "n_tot = 0\n",
    "n_zero = 0\n",
    "\n",
    "# Get the number of particles that don't contribute\n",
    "for i in range(N_SPOTS):\n",
    "    n_zero += np.all(detections[i,:,:] == 0)\n",
    "print(\"%d zero contributions\" % n_zero)\n",
    "print(\"%d out of %d particles contribute intensities to all spots\" % (t.topology.n_residues*N_SPOTS - n_zero, t.topology.n_residues*N_SPOTS))\n",
    "\n",
    "\n",
    "# # Get total number of nonzero contributions\n",
    "# for spot_num in range(N_SPOTS):\n",
    "#     for particle in detections[spot_num]:\n",
    "#         _nonzero = [x for x in particle if not x == 0.0 ]\n",
    "#         n_nonzero += len(_nonzero)\n",
    "#         n_tot += len(particle)\n",
    "    \n",
    "# # Get total number of detections\n",
    "# # n_detections = N_SPOTS * t.topology.n_residues * len(t)\n",
    "\n",
    "# print(\"Average percentage of the %d sampled particles that contribute to intensity is %f%%\" % (t.topology.n_residues, 100 * n_nonzero / n_tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intensity Traces\n",
    "Plot the intensity traces for the individual lipids, and for the summed intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:39.385130Z",
     "start_time": "2019-05-23T14:52:39.303779Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"intensities\", binned_dI[6][0])\n",
    "print(binned_dI[6][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T15:13:03.558425Z",
     "start_time": "2019-05-23T15:13:03.354123Z"
    }
   },
   "outputs": [],
   "source": [
    "################## Plot intensity traces #############################\n",
    "# Plot the intensities of detections associated with each lipid\n",
    "# plt.subplot(221)\n",
    "# # for _data in binned_dI:\n",
    "# #     plt.plot(np.arange(0, len(t), 1), _data, marker='o', markersize=.5 ) \n",
    "# plt.xlabel(\"Time (ns)\")\n",
    "# plt.ylabel(\"Intensity\")\n",
    "# plt.title(\"Intensity Trace\")\n",
    "\n",
    "# Plot sum of intensities (i.e. what a detector would see)\n",
    "plt.subplot(221)\n",
    "\n",
    "\n",
    "# for spot_num in range(N_SPOTS):\n",
    "#     plt.plot(np.arange(0, len(t), 1), binned_tot[spot_num], marker='None', label=\"Spot %d\" % spot_num)\n",
    "# plt.plot(binned_dI[6][0][17700:19600], marker='None', label=\"Spot %d\" % spot_num)\n",
    "plt.plot(binned_tots[6][0][11500:12500], marker='None', label=\"Spot %d\" % spot_num)\n",
    "    \n",
    "    \n",
    "# plt.plot(np.arange(0, len(t), 1), binned_tot[0], marker='None', markersize=.5 ) \n",
    "# summed_data = np.sum(intensities, axis=0)\n",
    "# #plt.plot(np.arange(0,len(summed_data), 1), summed_data, marker = 'o', markersize=.1, )\n",
    "# plt.plot(np.arange(0,len(binned), 1), [x[1] for x in binned], marker = 'o', markersize=.1, )\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time (ns)\")\n",
    "plt.ylabel(\"Intensity\")\n",
    "plt.title(\"Summed Intensity Trace\");\n",
    "\n",
    "\n",
    "\n",
    "# Attempts at filtering with wavelets\n",
    "# import pywt\n",
    "# wavelet = pywt.Wavelet('haar')\n",
    "# cA = pywt.downcoef('a', binned_dI[6][0][:15000], 'db9')\n",
    "\n",
    "# v2 = pywt.idwt(cA, np.zeros(len(cA)), 'db1')\n",
    "# plt.subplot(222)\n",
    "# plt.title('Approximation coefficients')\n",
    "\n",
    "# plt.plot(pywt.downcoef('a', binned_dI[6][0][:15000], 'db9', level=5), color='blue')\n",
    "\n",
    "# # ca2 = pywt.part(binned_dI[6][0][:15000], 'db9')\n",
    "# plt.plot(np.linspace(1,485,len(cA)),cA, color='r')\n",
    "# print(len(cD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing out transits\n",
    "\n",
    "It seems like if you don't specifically window around one transit event, the long lag time anticorrelations pop up.\n",
    "So, I want to come up with a way to window out a single transit event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T16:30:21.420035Z",
     "start_time": "2019-05-23T16:30:21.001240Z"
    }
   },
   "outputs": [],
   "source": [
    "RANGE_BOUND = 200\n",
    "\n",
    "_intensity = binned_tots[6][0]\n",
    "\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(_intensity, marker='.', ls='None')\n",
    "plt.plot(np.arange(11500,12500), _intensity[11500:12500], ls='None', marker='.', label=\"Spot %d\" % spot_num)\n",
    "\n",
    "_x = np.arange(len(binned_tots[6][0]))\n",
    "nonzero = _intensity[_intensity > 0]\n",
    "_nonzerox = _x[_intensity>0]\n",
    "print(len(nonzero))\n",
    "print(_nonzerox)\n",
    "\n",
    "# plt.plot(_nonzerox, nonzero, ls=\"None\", marker='.')\n",
    "\n",
    "\n",
    "# Get the nonzero ranges\n",
    "ranges = []\n",
    "_cur = []\n",
    "_start = None\n",
    "for idx in range(1, len(_nonzerox)):\n",
    "    \n",
    "    # Check if the current position is adjacent to the previous\n",
    "#     if _nonzerox[idx] - _nonzerox[idx-1] < RANGE_BOUND:\n",
    "#         _cur.append(_nonzerox[idx])\n",
    "    if _nonzerox[idx] - _nonzerox[idx-1] < RANGE_BOUND:\n",
    "        if _start is None:\n",
    "            _start = _nonzerox[idx]\n",
    "        else:\n",
    "            _end = _nonzerox[idx]\n",
    "    \n",
    "    # If you reach a zero value while in a nonzero range \n",
    "    elif _nonzerox[idx] - _nonzerox[idx-1] > RANGE_BOUND:\n",
    "        \n",
    "        print(\"Found a range of nonzeros from %d to %d\" % (_start, _end))\n",
    "        \n",
    "        ranges.append(np.arange(_start, _end))\n",
    "        _start = None\n",
    "        _end = None\n",
    "        \n",
    "    else:\n",
    "        print(\"%s %s ?\" % (idx, _nonzerox[idx]))\n",
    "        \n",
    "print(ranges)\n",
    "for _range in ranges:\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(_range, _intensity[_range], '.')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:39.955654Z",
     "start_time": "2019-05-23T14:52:39.953698Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(0, len(t), 1), binned_tot[11], marker='None', label=\"Spot %d\" % spot_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Autocorrelations\n",
    "Plot the autocorrelation functions for the individual lipids tracked, and for the summed intensities of all of them."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T21:22:54.308992Z",
     "start_time": "2018-03-31T21:08:18.941439Z"
    },
    "hidden": true
   },
   "source": [
    "# Don't set the max lag to greater than the number of datapoints..\n",
    "# max_lag = min([100 * int(tauD), (len(t) -1) * STEP])\n",
    "max_lag = (len(t)-1) * STEP\n",
    "\n",
    "tauD = FWHM**2 / (4*D)\n",
    "\n",
    "# # Individual data\n",
    "# plt.subplot(221)\n",
    "# plt.xscale('log')\n",
    "\n",
    "# # for _data in binned_dI:\n",
    "# #     plt.acorr(_data, maxlags=max_lag, usevlines=False, linestyle='-', marker=\"None\", normed=True)\n",
    "\n",
    "# plt.xlabel(\"Time lag (ns)\")\n",
    "# plt.title(\"Normalized Autocorrelation\")\n",
    "\n",
    "# # Plot model curve\n",
    "# _x = np.arange(0,len(t), 1)\n",
    "# _G =(1 + _x / tauD)**(-1)\n",
    "# plt.plot(_x, _G, linestyle='--', linewidth=4)\n",
    "    \n",
    "# # Plot tau_D, diffusion time\n",
    "# plt.axvline(tauD)\n",
    "# plt.xticks(list(plt.xticks()[0]) + [tauD], list(plt.xticks()[0]) + ['tau_D'])\n",
    "# plt.xlim([0,max_lag])\n",
    "\n",
    "\n",
    "# Summed data\n",
    "plt.subplot(222)\n",
    "plt.xscale('log')\n",
    "\n",
    "_x = np.arange(0, len(t), 1)\n",
    "print(\"Beginning analysis\")\n",
    "for spot_num in range(N_SPOTS):\n",
    "    print(\"Analyzing spot %d\" % spot_num, end=\"\\r\")\n",
    "    autocorrelated = autocorrelate(binned_tot[spot_num])\n",
    "    plt.plot(_x, autocorrelated, label=\"Data - Spot %d\" % spot_num)\n",
    "#     plt.acorr(binned_tot[spot_num], maxlags=max_lag, usevlines=False, linestyle='-', marker=\"None\", normed=True, label=\"Data - Spot %d\" % spot_num)\n",
    "\n",
    "\n",
    "# Plot model curve\n",
    "_x = np.arange(0,len(t), 1)\n",
    "# _G = (1 + _x / tauD)**(-1)\n",
    "plt.plot(_x, autocorr_model(_x, D*1), linestyle='--', color='k', linewidth=4, label=\"Model\")\n",
    "\n",
    "plt.xlabel(\"Time lag (ns)\")\n",
    "plt.title(\"Normalized Autocorrelation\")\n",
    "    \n",
    "# Plot tau_D, diffusion time\n",
    "plt.axvline(tauD)\n",
    "plt.xticks(list(plt.xticks()[0]) + [tauD], list(plt.xticks()[0]) + ['tau_D'])\n",
    "plt.xlim([0,max_lag])\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine diffusion coefficient from FCS data\n",
    "\n",
    "The following cells determine the diffusion coefficient from the FCS data using two different techniques. The binned and summed data are treated separately. First, a curve is fit to the autocorrelation curve using the diffusion constant as the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curve Fit Method\n",
    "\n",
    "Using the equation for diffusion in a membrane presented by Schwille$^{[1]}$, attempt to fit the FCS data to a function of the form\n",
    "\n",
    "$G(t) = \\frac{1}{N} \\left(1 + \\frac{t}{\\tau_D} \\right)^{-1}$, where\n",
    "\n",
    "$\\tau_D = \\frac{w_{xy}^2}{4 D}$\n",
    "\n",
    "Since the autocorrelation curves are normalized, $N$ is set to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Binned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:39.968463Z",
     "start_time": "2019-05-23T14:52:39.956838Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# _x = np.arange(0, len(t), 1)\n",
    "\n",
    "# plt.xscale('log')\n",
    "# optimals = []\n",
    "# covariances = []\n",
    "# i = 0\n",
    "# for _data in binned_dI:\n",
    "#     i+= 1\n",
    "#     print(i, end=\"\\r\")\n",
    "    \n",
    "#     autocorrelated = autocorrelate(_data)\n",
    "    \n",
    "#     _optimal, _covariance = curve_fit(autocorr_model, _x, autocorrelated, p0=D)\n",
    "#     optimals.append(_optimal)\n",
    "#     covariances.append(_covariance)\n",
    "    \n",
    "#     plt.plot(_x, autocorrelated)\n",
    "#     plt.plot(_x, autocorr_model(_x, _optimal), linestyle='--')\n",
    "    \n",
    "# var = [np.sqrt(np.diag(x)) for x in covariances]\n",
    "    \n",
    "# print(\"Average D from data was %f +- %f\" % (np.mean(optimals), np.mean(var)))\n",
    "# plt.xlim([0,len(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Summed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:39.985420Z",
     "start_time": "2019-05-23T14:52:39.969928Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# _D = []\n",
    "\n",
    "# for spot_num in range(len(binned_tot)):\n",
    "\n",
    "#     autocorrelated = autocorrelate(binned_tot[spot_num])\n",
    "\n",
    "#     optimal, covariance = curve_fit(autocorr_model, _x, autocorrelated, p0=D)\n",
    "    \n",
    "#     _D.append(p0)\n",
    "\n",
    "#     print(\"D was set at %f\" % D)\n",
    "#     print(\"D estimated at %f +- %f.\" % (optimal, np.sqrt(np.diag(covariance))))\n",
    "\n",
    "#     plt.xscale('log')\n",
    "\n",
    "#     # Orange line is using the known diffusion constant\n",
    "#     plt.plot(_x, autocorr_model(_x, D), linestyle='--', label=\"Model - Known D\")\n",
    "#     plt.acorr(binned_tot[0], maxlags=49999, usevlines=False, linestyle='-', marker=\"None\", normed=True, label=\"Data\")\n",
    "\n",
    "#     # Black line is using the calculated diffusion constant\n",
    "#     plt.plot(_x, autocorr_model(_x, optimal), linestyle='--', color='black', label=\"Model - Calced D\")\n",
    "#     var = np.sqrt(np.diag(covariance))\n",
    "\n",
    "# #     _y1 = autocorr_model(_x, optimal+var)\n",
    "# #     _y2 = autocorr_model(_x, optimal-var)\n",
    "\n",
    "# # plt.fill_between(_x, _y1, _y2)\n",
    "\n",
    "# plt.xlim([0,len(t)])\n",
    "# plt.legend()\n",
    "\n",
    "# print(_D)\n",
    "# print(np.mean(_D))\n",
    "# print(\"D was {}+/-{}\".format(np.mean(_D), np.std(_D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .5 crossing method\n",
    "This method determines $\\tau_D$ first by looking for where the normalized autocorrelation crosses 0.5, then computes the diffusion coefficient from that using the same formula as above, solved for $D$\n",
    "\n",
    "$D = \\frac{w_{xy}^2}{4 \\tau_D}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:40.004278Z",
     "start_time": "2019-05-23T14:52:39.986904Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binned_dI.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T15:12:47.080983Z",
     "start_time": "2019-05-23T15:12:46.524786Z"
    }
   },
   "outputs": [],
   "source": [
    "autocorrelated = []\n",
    "timestep = 1\n",
    "_x = np.arange(len(t))\n",
    "print(\"Len is %d\" % len(binned_dI[6][0]))\n",
    "# _x = np.arange(len(cA))\n",
    "# _x = np.arange(0,5000)\n",
    "reduction=1\n",
    "newlen = len(binned_dI[6][0])//reduction\n",
    "# newlen=2000//reduction\n",
    "newlen=500\n",
    "_x = np.arange(0, newlen)\n",
    "\n",
    "_x = np.array([x * timestep for x in _x])\n",
    "# print(_x[:10])\n",
    "# print(_x[-10:])\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "crossings = []\n",
    "\n",
    "print(\"Beginning analysis\")\n",
    "plt.xscale('log')\n",
    "spot_num = 6\n",
    "# for spot_num in range(N_SPOTS):\n",
    "############ indent below\n",
    "for _data in binned_dI[spot_num]:\n",
    "\n",
    "#     _data = cA\n",
    "#     _data = None\n",
    "#     _data = _data[11000:13000]\n",
    "\n",
    "    print(\"Analyzing spot %d\" % spot_num, end=\"\\r\")\n",
    "\n",
    "    # If there are no intensity contributions, skip this\n",
    "    if np.count_nonzero(_data) == 0:\n",
    "        continue\n",
    "\n",
    "    autocorrelated = autocorrelate(_data[11500:12000], normed=True, reduction=reduction,step=1)\n",
    "\n",
    "    try:\n",
    "        crossings.append(get_crossing(_x, autocorrelated, tauD))\n",
    "        crossings[-1] = crossings[-1]*timestep\n",
    "\n",
    "\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"Bad guess. Skipping a bin in spot %d\" % (spot_num))\n",
    "        continue\n",
    "\n",
    "    color = ax._get_lines.get_next_color()\n",
    "\n",
    "#     calced_D = FWHM**2 / (4 * crossings[-1])\n",
    "\n",
    "    plt.plot(_x, autocorrelated, color=color, label=\"Spot {} - Calculated\".format(spot_num))\n",
    "#     plt.plot(_x, autocorr_model(_x, calced_D), color=color, linestyle='--', label=\"Spot {} - D Model\".format(spot_num))\n",
    "################### indent above\n",
    "\n",
    "\n",
    "\n",
    "print(crossings)\n",
    "# calced_D = [FWHM**2 / (4 * x) for x in crossings]\n",
    "mean = 10**(np.nanmean(np.log10(crossings)))\n",
    "mean = np.nanmean(crossings)\n",
    "calced_D = FWHM**2 / (4 * mean)\n",
    "\n",
    "\n",
    "D=.13\n",
    "\n",
    "plt.plot(_x, autocorr_model(_x, D), color='k', linestyle='-', linewidth=2, label=\"Model - Expected\")\n",
    "plt.plot(_x, autocorr_model(_x, calced_D), color='k', linestyle='--', linewidth=2, label=\"Model - Calculated\")\n",
    "\n",
    "print(\"Model says %s \" % autocorr_model(_x,calced_D)[:10])\n",
    "plt.legend()    \n",
    "## Plot tau_D, diffusion time\n",
    "calc_tauD = FWHM**2 / (4 * np.mean(calced_D))\n",
    "plt.axvline(calc_tauD)\n",
    "plt.axvline(tauD)\n",
    "plt.axhline(0)\n",
    "\n",
    "plt.xticks(list(plt.xticks()[0]) + [calc_tauD, tauD], list(plt.xticks()[0]) + ['calculated tau_D', 'expected tau_D'])\n",
    "\n",
    "plt.xlim([0,len(t)])\n",
    "\n",
    "print(\"Average tau_D is %f +- %f\" % (np.nanmean(crossings), np.nanstd(crossings)))\n",
    "print(\"Average diffusion constant is %.3e +- %.4e\" % (np.nanmean(calced_D), FWHM**2 / (4 * np.nanstd(crossings))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summed data for each spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:41.580095Z",
     "start_time": "2019-05-23T14:52:37.516Z"
    }
   },
   "outputs": [],
   "source": [
    "zero_array = np.zeros(len(binned_tot[0])//5)\n",
    "print(len(zero_array))\n",
    "\n",
    "# Cut out datasets that are more than MAX_PERCENTAGE zeros\n",
    "MAX_PERCENTAGE = 0.9\n",
    "max_zeros = int(len(binned_tot[0])*MAX_PERCENTAGE)\n",
    "print(max_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:41.580673Z",
     "start_time": "2019-05-23T14:52:37.532Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.xscale('log')\n",
    "ax = plt.gca()\n",
    "_x = np.arange(0, len(t), 1)\n",
    "\n",
    "Ds = []\n",
    "\n",
    "skipped = 0\n",
    "for spot_num in range(N_SPOTS):\n",
    "    \n",
    "    \n",
    "    print(\"--- Spot %d ---              \" % spot_num, end=\"\\r\")\n",
    "    \n",
    "#     if np.sum(binned_tot[spot_num]) == 0.0:\n",
    "#         print(\"This spot is all 0, skipping\", end=\"\")\n",
    "#         skipped += 1\n",
    "#         continue\n",
    "        \n",
    "# # #     print(binned_tot[spot_num])\n",
    "# # #     print(\"%d zeros\" % (len(binned_tot[spot_num]) - np.count_nonzero(binned_tot[spot_num])))\n",
    "    \n",
    "#     if (len(binned_tot[spot_num]) - np.count_nonzero(binned_tot[spot_num])) > max_zeros:\n",
    "# #         print(\"Skipping - %d zeros\" % (len(autocorrelated) - np.count_nonzero(autocorrelated)))\n",
    "#         skipped += 1\n",
    "#         continue\n",
    "    \n",
    "#     print(binned_dI.shape)\n",
    "    autocorrelated = autocorrelate(binned_tot[spot_num])\n",
    "    \n",
    "    # Truncate small numbers to 0 to make the zero_array check faster\n",
    "    autocorrelated[np.abs(autocorrelated) < 1e-2] = 0\n",
    "#     print(autocorrelated)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        crossing = get_crossing(_x, autocorrelated, 1)\n",
    "    except ValueError:\n",
    "        print(\"Bad result. Skipping a bin in spot %d\" % (spot_num))\n",
    "        continue\n",
    "        \n",
    "#     if (len(autocorrelated) - np.count_nonzero(autocorrelated)) > max_zeros:\n",
    "#         print(\"Skipping - %d zeros\" % (len(autocorrelated) - np.count_nonzero(autocorrelated)))\n",
    "#         skipped += 1\n",
    "#         continue\n",
    "    \n",
    "    \n",
    "#     print(\"Crossing is at %f\" % crossing)\n",
    "\n",
    "    calced_D = FWHM**2 / (4 * crossing)\n",
    "\n",
    "    color = ax._get_lines.get_next_color()\n",
    "    plt.plot(_x, autocorrelated, color=color, label=\"Data - Spot %d\" % spot_num)\n",
    "    plt.plot(_x, autocorr_model(_x, calced_D), color=color, linestyle='--', label=\"Model - Spot %d\" % spot_num)\n",
    "\n",
    "    print(\"\\nCalculated D is %.3e, tau_D is %.3e\" %( calced_D, crossing))\n",
    "    \n",
    "    Ds.append(calced_D)\n",
    "\n",
    "print(\"\\nSkipped %d out of %d\" % (skipped, N_SPOTS))\n",
    "\n",
    "plt.plot(_x, autocorr_model(_x, D), color='k', linestyle='-', linewidth=2, label=\"Model - Expected\")\n",
    "    \n",
    "# Plot tau_D, diffusion time\n",
    "\n",
    "print(FWHM**2 / (4 * np.array(Ds)))\n",
    "calc_tauD = FWHM**2 / (4 * np.mean(Ds))\n",
    "print(\"\\n\\nAvg. D is %.3e +- %.3e, yielding a tau_D of %.2e compared to the expected %.2e for D=%d\" % (np.mean(Ds), np.std(Ds), calc_tauD, tauD, D))\n",
    "\n",
    "plt.axvline(calc_tauD)\n",
    "plt.axvline(tauD)\n",
    "\n",
    "plt.xticks(list(plt.xticks()[0]) + [calc_tauD, tauD], list(plt.xticks()[0]) + ['calculated tau_D', 'expected tau_D'])\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "plt.xlim([0,len(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average autocorrelations for each spot\n",
    "All autocorrelations are averaged together, then 0 crossing is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:41.581456Z",
     "start_time": "2019-05-23T14:52:37.545Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.xscale('symlog', linscale=0.1)\n",
    "plt.xlim([0,1E5])\n",
    "ax = plt.gca()\n",
    "_x = np.arange(0, len(t), 1)\n",
    "\n",
    "autocorrelated = np.ndarray(shape=(N_SPOTS,len(t)))\n",
    "\n",
    "# Average autocorrelations for each spot\n",
    "for spot_num in range(N_SPOTS):\n",
    "    _acorr = []\n",
    "    for _bin in range(len(binned_dI[0])):\n",
    "    \n",
    "        if np.count_nonzero(binned_dI[spot_num][_bin]) == 0:\n",
    "            print(\"Spot/bin %d/%d is all 0, setting to NaN             \" % (spot_num, _bin), end='\\r')\n",
    "\n",
    "            # If the spot didn't capture any intensity contributions, just set it to NaN.\n",
    "            # NaN values will be ignored when taking the mean. (Thanks, np.nanmean)\n",
    "\n",
    "            autocorrelated[spot_num] = np.full(len(t), np.NAN)\n",
    "            continue\n",
    "\n",
    "        _acorr.append(autocorrelate(binned_dI[spot_num][_bin]))\n",
    "        print(\"Parsed spot %d - bin %d           \" % (spot_num, _bin), end='\\r')\n",
    "        \n",
    "    autocorrelated[spot_num] = np.nanmean(_acorr, axis=0)\n",
    "\n",
    "print(autocorrelated)\n",
    "# print(np.mean(autocorrelated, axis=0))\n",
    "    \n",
    "autocorrelated = np.nanmean(autocorrelated, axis=0)\n",
    "\n",
    "print(autocorrelated)\n",
    "\n",
    "\n",
    "crossing = get_crossing(_x, autocorrelated, tauD)\n",
    "\n",
    "calced_D = FWHM**2 / (4 * crossing)\n",
    "\n",
    "print(\"Calculated D at %.2e\" % calced_D)\n",
    "print(\"Calibrated to %f\" % calibrate(ALL_AVG, calced_D))\n",
    "\n",
    "plt.plot(_x, autocorrelated, label=\"Averaged spot data\")\n",
    "\n",
    "plt.plot(_x, autocorr_model(_x, calced_D), linestyle='-', linewidth=1, label=\"Model - Calculated\")\n",
    "\n",
    "plt.plot(_x, autocorr_model(_x, D), color='k', linestyle='--', linewidth=2, label=\"Model - Expected\")\n",
    "\n",
    "plt.axvline(crossing)\n",
    "plt.axvline(tauD)\n",
    "plt.axhline(0.0)\n",
    "plt.xticks(list(plt.xticks()[0]) + [crossing, tauD], list(plt.xticks()[0]) + ['calculated tau_D', 'expected tau_D'])\n",
    "print(\"Taud was %.4e\" % crossing)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# from matplotlib import rc\n",
    "# from matplotlib import rcParams\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "# image = PdfPages( ('averaged_acorr_d%.1f' % calced_D).replace('.','_') + '.pdf')\n",
    "# image.savefig()\n",
    "# image.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average autocorrelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:41.582227Z",
     "start_time": "2019-05-23T14:52:37.557Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.xscale('log')\n",
    "ax = plt.gca()\n",
    "_x = np.arange(0, len(t), 1)\n",
    "\n",
    "Ds = []\n",
    "\n",
    "skipped = 0\n",
    "_acorrs = []\n",
    "\n",
    "for spot_num in range(N_SPOTS):\n",
    "    \n",
    "    \n",
    "    print(\"--- Spot %d ---              \" % spot_num, end=\"\\r\")\n",
    "    \n",
    "    autocorrelated = autocorrelate(binned_tot[spot_num])\n",
    "    \n",
    "    # Truncate small numbers to 0 to make the zero_array check faster\n",
    "    autocorrelated[np.abs(autocorrelated) < 1e-2] = 0\n",
    "    \n",
    "    _acorrs.append(autocorrelated)\n",
    "\n",
    "    color = ax._get_lines.get_next_color()\n",
    "    plt.plot(_x, autocorrelated, color=color, label=\"Data - Spot %d\" % spot_num)\n",
    "#     plt.plot(_x, autocorr_model(_x, calced_D), color=color, linestyle='--', label=\"Model - Spot %d\" % spot_num)\n",
    "\n",
    "#     print(\"\\nCalculated D is %.3f, tau_D is %.3f\" %( calced_D, crossing))\n",
    "    \n",
    "#     Ds.append(calced_D)\n",
    "    \n",
    "# Average together autocorrelations\n",
    "\n",
    "# print(np.mean(_acorrs, axis=0))\n",
    "\n",
    "_acorrs = np.nanmean(_acorrs, axis=0)\n",
    "print(_acorrs)\n",
    "\n",
    "plt.plot(_x, _acorrs, color='k', linestyle='-', label=\"Avg Acorr\")\n",
    "\n",
    "\n",
    "try:\n",
    "    crossing = get_crossing(_x, _acorrs, 1)\n",
    "except ValueError:\n",
    "    print(\"Bad result. Skipping a bin in spot %d\" % (spot_num))\n",
    "    \n",
    "    print('')\n",
    "\n",
    "calced_D = FWHM**2 / (4 * crossing)\n",
    "\n",
    "print(\"Calculated D=%.2e\" % calced_D)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nSkipped %d out of %d\" % (skipped, N_SPOTS))\n",
    "\n",
    "plt.plot(_x, autocorr_model(_x, D), color='k', linestyle='--', linewidth=2, label=\"Model - Expected\")\n",
    "    \n",
    "\n",
    "calc_tauD = FWHM**2 / (4 * calced_D)\n",
    "print(\"Calculated tauD at %.2f\" % calc_tauD)\n",
    "# print(\"\\n\\nAvg. D is %.3f +- %.4f, yielding a tau_D of %.2f compared to the expected %.2f for D=%d\" % (np.mean(Ds), np.std(Ds), calc_tauD, tauD, D))\n",
    "\n",
    "plt.axvline(calc_tauD)\n",
    "plt.axvline(tauD)\n",
    "\n",
    "plt.xticks(list(plt.xticks()[0]) + [calc_tauD, tauD], list(plt.xticks()[0]) + ['calculated tau_D', 'expected tau_D'])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim([0,len(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelate in bins, and then curve fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Group trajectory autocorrelations into bins, then average each bin's data separately\n",
    "calced_Ds[BIN, SPOT] is filled with the 0.5 crossings for each trajectory. These are then averaged bin-wise"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T20:21:28.490794Z",
     "start_time": "2018-04-02T20:21:26.962292Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "source": [
    "# plt.xscale('symlog', linscale=0.1)\n",
    "# plt.xlim([0,1E5])\n",
    "# ax = plt.gca()\n",
    "_x = np.arange(0, len(t), 1)\n",
    "\n",
    "NUM_B = 4 # Number of bins to autocorrelate into before curvefit\n",
    "TPB = N_SPOTS//NUM_B # Number of spots per bin\n",
    "\n",
    "calced_Ds = np.full(shape=(NUM_B,TPB), fill_value=np.nan)\n",
    "crossings = np.full(shape=(NUM_B,TPB), fill_value=np.nan)\n",
    "\n",
    "for _bin in range(NUM_B):\n",
    "    print(\"\\nAnalyzing bin %d\" % _bin)\n",
    "    \n",
    "    autocorrelated = np.ndarray(shape=(TPB, len(t)))\n",
    "\n",
    "    # Average autocorrelations for each bin\n",
    "    for spot_num in range(TPB * _bin, TPB * (_bin + 1)):\n",
    "        \n",
    "        if np.count_nonzero(binned_dI[spot_num][0]) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(\"Analyzing spot %d\" % spot_num, end=\"\\r\")\n",
    "        \n",
    "        autocorrelated = autocorrelate(binned_dI[spot_num][0])\n",
    "    \n",
    "        try:\n",
    "            crossings[_bin, spot_num % TPB] = get_crossing(_x, autocorrelated, tauD)\n",
    "            if spot_num < 50:\n",
    "#                 plt.plot(_x, autocorrelated, label=\"Spot %d\" % spot_num)\n",
    "                print(\"Spot %d crossing is %d\" % (spot_num, crossings[_bin, spot_num % TPB]))\n",
    "            \n",
    "            \n",
    "        except ValueError:\n",
    "#             print(\"Bad guess. Skipping a bin in spot %d\" % (spot_num))\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            calced_D = FWHM**2 / (4 * crossings[_bin, spot_num % TPB])\n",
    "            \n",
    "            if calced_D < 1e-3:\n",
    "                calced_D = np.nan\n",
    "                print(\"Bad crossing was %f\" % crossings[_bin, spot_num % TPB])\n",
    "                \n",
    "            calced_Ds[_bin, spot_num % TPB] = calced_D\n",
    "# print(calced_Ds)\n",
    "                \n",
    "    \n",
    "# print(crossings)\n",
    "print(\"Average tauD is %s\" % np.nanmean(crossings, axis=1))\n",
    "\n",
    "data = np.nanmean(calced_Ds, axis=1)\n",
    "calibrated = calibrate(CROSSING, np.nanmean(calced_Ds, axis=1))\n",
    "print(\"Average D is %s\" % data)\n",
    "print(\"Calibrated D is %s\" % calibrated)\n",
    "print(\"Before calibration, D = %.3f +- %.3f\" % (np.mean(data), np.std(data)))\n",
    "print(\"Using calibrated data, D = %.3f +- %.3f\" % (np.mean(calibrated), np.std(calibrated)))\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Average binned autocorrelations\n",
    "Autocorrelations are first averaged bin-wise, then the averages are curve-fit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T20:01:09.315733Z",
     "start_time": "2017-08-22T19:57:48.510271Z"
    },
    "hidden": true
   },
   "source": [
    "_x = np.arange(0, len(t), 1)\n",
    "\n",
    "NUM_B = 4 # Number of bins to autocorrelate into before curvefit\n",
    "TPB = N_SPOTS//NUM_B # Number of spots per bin\n",
    "\n",
    "autocorrelated = np.full(shape=(NUM_B,len(t)), fill_value=np.nan)\n",
    "\n",
    "\n",
    "for _bin in range(NUM_B):\n",
    "    print(\"\\nAnalyzing bin %d\" % _bin)\n",
    "    \n",
    "    bin_autocorrelated = np.full(shape=(TPB, len(t)), fill_value=np.nan)\n",
    "    \n",
    "    # Average autocorrelations for each spot\n",
    "    for spot_num in range(TPB):\n",
    "        \n",
    "        print(\"Analyzing spot %d\" % spot_num, end=\"\\r\")\n",
    "\n",
    "        if np.count_nonzero(binned_dI[spot_num + TPB*_bin]) == 0:\n",
    "    #         print(\"Spot %d is all 0, setting to NaN\" % spot_num)\n",
    "\n",
    "            # If the spot didn't capture any intensity contributions, just set it to NaN.\n",
    "            # NaN values will be ignored when taking the mean. (Thanks, np.nanmean)\n",
    "            bin_autocorrelated[spot_num] = np.full(len(t), np.NAN)\n",
    "            continue\n",
    "#         print(\"Nonzero spot\")\n",
    "        bin_autocorrelated[spot_num] = autocorrelate(binned_dI[spot_num + TPB * _bin][0])\n",
    "#         print(binned_dI[spot_num + TPB* _bin])\n",
    "#         print(bin_autocorrelated[spot_num])\n",
    "\n",
    "#     print(bin_autocorrelated.shape)\n",
    "    autocorrelated[_bin] = np.nanmean(bin_autocorrelated, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Curve-fit each bin's data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T20:01:13.677232Z",
     "start_time": "2017-08-22T20:01:09.316875Z"
    },
    "hidden": true
   },
   "source": [
    "plt.xscale('symlog', linthresh=1)\n",
    "optimals = []\n",
    "covariances = []\n",
    "\n",
    "for _data in autocorrelated:\n",
    "    \n",
    "    _autocorrelated = autocorrelate(_data)\n",
    "    \n",
    "    _optimal, _covariance = curve_fit(autocorr_model, _x, _autocorrelated, p0=D)\n",
    "    optimals.append(_optimal)\n",
    "    covariances.append(_covariance)\n",
    "    \n",
    "    plt.plot(_x, _autocorrelated)\n",
    "    plt.plot(_x, autocorr_model(_x, _optimal), linestyle='--')\n",
    "    \n",
    "\n",
    "plt.plot(_x, autocorr_model(_x, D), color='k', linestyle='--', linewidth=2, label=\"Model - Expected\")\n",
    "plt.axhline(0.0)\n",
    "    \n",
    "# errors = []\n",
    "# for cov in covariances:\n",
    "#     errors.append(np.sqrt(np.diag(cov)))\n",
    "\n",
    "corrected = calibrate(CURVE_FIT, optimals)  \n",
    "\n",
    "print(optimals)\n",
    "print(\"Average D from curve fit was %f +- %f\" % (np.mean(optimals), np.std(optimals)))\n",
    "print(corrected)\n",
    "print(\"Corrected D from curve fit was %f +- %f\" % (np.mean(corrected), np.std(corrected)))\n",
    "\n",
    "# print(\"Covariances gave errors of %s\" % errors)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T19:21:16.592783Z",
     "start_time": "2017-08-23T19:21:16.582921Z"
    },
    "hidden": true
   },
   "source": [
    "print(optimals)\n",
    "corrected = calibrate(CURVE_FIT, optimals)\n",
    "print(corrected)\n",
    "print(np.mean(corrected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this is to plot the spots overlaid on the trajectories of the sampled particles, with a heatmap indicating the Gaussian profile that's being used to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:41.583166Z",
     "start_time": "2019-05-23T14:52:37.590Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the aspect ratio of the plot square so that the spot circles are circles and not ellipses\n",
    "plt.axis('equal') \n",
    "\n",
    "\n",
    "# spot_centers = np.array([ [np.random.uniform(700,1200), np.random.uniform(1700,2300)] for x in range(1000)])\n",
    "# spot_centers = np.array([[700,1700],[1020,1800],[940,2000],[1100,2200],[830,2100]])\n",
    "\n",
    "try:\n",
    "    spot_centers\n",
    "except NameError:    \n",
    "    print(\"Error - no centers found\")\n",
    "    spot_centers = [[0,0]]\n",
    "############## Plot trajectories ############## \n",
    "for w in range(0, t.topology.n_residues, 1):\n",
    "    plt.plot(t.xyz[:,w,0], t.xyz[:,w,1], linestyle='-', linewidth=.2, zorder=1, alpha=.6)\n",
    "\n",
    "\n",
    "############## Plot spots ############## \n",
    "ax = plt.gca()\n",
    "for spot in spot_centers[:N_SPOTS]:\n",
    "    circle = plt.Circle(spot, FWHM/2, fill=False, linewidth=2, ec='white', zorder=3)\n",
    "    ax.add_artist(circle)\n",
    "    \n",
    "    \n",
    "############## Plot heatmap ############## \n",
    "# Get axis ranges to use for heatmap point grid\n",
    "xrange = [1.45,1.65]#ax.get_xlim()\n",
    "yrange = [2,2.4]#ax.get_ylim()\n",
    "# xrange = [1200, 2665]\n",
    "# yrange = [1900,2984]\n",
    "# xrange = [-SPOT_RANGE,SPOT_RANGE]#ax.get_xlim()\n",
    "# yrange = [-SPOT_RANGE,SPOT_RANGE]#ax.get_ylim()\n",
    "plt.x_range = xrange\n",
    "\n",
    "# Set the number of points. Make this imaginary so that np.mgrid includes the endpoints.\n",
    "npoints = 1500j\n",
    "zpreds = []\n",
    "\n",
    "# Make a grid of points that will be used to display the Gaussians. May also be able to do this as a countour..\n",
    "yi, xi = np.mgrid[yrange[1]:yrange[0]:npoints, xrange[0]:xrange[1]:npoints]\n",
    "xyi = np.vstack([xi.ravel(), yi.ravel()])\n",
    "\n",
    "# Determine Gaussian profiles for each spot\n",
    "\n",
    "for spot in spot_centers[:N_SPOTS]:\n",
    "    zpred = gauss2d(xyi, spot, w_xy, cutoff=CUTOFF)\n",
    "    zpred.shape = xi.shape # Convert Z values from a flat list to a 2-D (x,y) array\n",
    "    zpreds.append(zpred)\n",
    "#     plt.annotate(xy=spot, s=\"Spot\", color='w') # The `where` is some magic to get the index of the spot\n",
    "# # If using multiple spots, uncomment this and comment the line above\n",
    "#     plt.annotate(xy=spot, s=\"Spot %d\" % np.where(spot_centers==spot)[0][0], color='w', fontsize=14) # The `where` is some magic to get the index of the spot\n",
    "    \n",
    "# Set each Z value to the max at that position, most meaningful way of plotting multiple profiles (?)\n",
    "zpreds = np.maximum.reduce(zpreds)\n",
    "\n",
    "# Display Gaussian profiles\n",
    "im = ax.imshow(zpreds, extent=[xi.min(), xi.max(), yi.min(), yi.max()], aspect='equal', zorder=0, alpha=1, cmap='magma')\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label(\"Gaussian profile - Intensity Contribution\",size=28)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"x (nm)\")\n",
    "plt.ylabel(\"y (nm)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig('gauss.png')\n",
    "\n",
    "from matplotlib import rc\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'font.size': 28})\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# image = PdfPages('gauss.pdf')\n",
    "# image.savefig()\n",
    "# image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:41.583990Z",
     "start_time": "2019-05-23T14:52:37.604Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max(t.xyz[:,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T14:52:41.584642Z",
     "start_time": "2019-05-23T14:52:37.616Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # spot_centers = np.array([[ 206.67443525,  500.41622301],\n",
    "# #        [ 164.90190702,  542.62424512],\n",
    "# #        [ 184.46268823,  507.97702133],\n",
    "# #        [ 162.16366869,  500.24720175],\n",
    "# #        [ 165.1454897 ,  491.41992064],\n",
    "# #        [ 167.49883021,  511.29115524],\n",
    "# #        [ 183.70589247,  518.67824831],\n",
    "# #        [ 171.03233886,  520.73700045],\n",
    "# #        [ 169.53397387,  533.68897864],\n",
    "# #        [ 193.72767373,  500.21576509]])*10\n",
    "\n",
    "# # spot_centers = np.array([[ 165.1454897 ,  491.41992064]])*10\n",
    "# # spot_centers = np.delete(spot_centers, [28, 14, 3, 30], 0)\n",
    "# spot_centers = all_centers\n",
    "# print(spot_centers)\n",
    "# len(spot_centers)\n",
    "# # spot_centers = [all_centers[x] for x in range(len(all_centers))]# if x not in [3, 28, 30, 34, 35, 31, 26, 20]]\n",
    "# # N_SPOTS = len(spot_centers)\n",
    "# # print(N_SPOTS)\n",
    "\n",
    "# # spot_centers = np.concatenate((spot_centers, np.array([ [np.random.uniform(600,1000), np.random.uniform(1700,2250)] for x in range(N_SPOTS)])), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Chiantia, Salvatore, Jonas Ries, and Petra Schwille. \"Fluorescence correlation spectroscopy in membrane structure elucidation.\" Biochimica et Biophysica Acta (BBA)-Biomembranes 1788.1 (2009): 225-233.\n",
    "\n",
    "[2] Zgorski, Andrew, and Edward Lyman. \"Toward Hydrodynamics with Solvent Free Lipid Models: STRD Martini.\" Biophysical journal 111.12 (2016): 2689-2697."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "942px",
    "left": "0px",
    "right": "2211.5px",
    "top": "66px",
    "width": "229px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}