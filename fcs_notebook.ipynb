{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual FCS, using MD Simulation Data\n",
    "\n",
    "This goal of this notebook is to generate a simulated FCS measurement using data from a GROMACS simulation.\n",
    "\n",
    "The setup of the virtual system is a membrane with a circularly symmetric incident beam.\n",
    "\n",
    "Data is read in from an .xtc or .trr file, using a .gro file to define the system topology. Data frames from the input file are iterated through, and for each frame, a detected intensity from each lipid is calculated. The intensity trace and autocorrelation functions for each lipid, and for the total at each frame, are plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "\n",
    "#### \u2612 Why don't autocorrelation curves look right? Flatness in middle is unexpected\n",
    "- Done. See below\n",
    " \n",
    "#### \u2612 Generate and plot autocorrelation curve prediction. I'm still not understanding something about autocorrelation curves - I don't see where the sigmoid shape comes from.\n",
    " - Done. Curves need to be plotted on a semilog scale in order to see the expected sigmoid shape.\n",
    " \n",
    "#### \u2612 Try autocorrelating the data myself instead of using acorr. Maybe something weird is happening with the way matplotlib does autocorrelation that isn't desirable.\n",
    " - Done. The manually autocorrelated data is identical to calling plt.acorr with normed=True\n",
    " \n",
    "#### \u2612 Parallelize data analysis\n",
    " - Tried it. The analysis is quite computationally cheap compared to the extra overhead from spawning new threads -- not worth the time saved by running the analysis in parallel.\n",
    " \n",
    "#### \u2612 Handle breaking data up into bins better -- currently ignores the remainder of lipids that don't fall into a bin. (I.e. 11 lipids, bin size 3, the remaining 2 that don't evenly fall into bins are discarded.)\n",
    " - Done. The leftover lipids are put into their own bin, which may be smaller than the BIN_SIZE.\n",
    " \n",
    "#### \u2612 How do PBC vs unwrapped affect prediction of diffusion constant?\n",
    " - Done. Don't appear to significant affect results.\n",
    " \n",
    "#### \u2610 Curve fit method seems to work better for small spot sizes, but 0.5-crossing method seems better for larger spots? Why?\n",
    "     \n",
    "[Checkbox symbols]:<> (\u2612 \u2610)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on input data\n",
    "\n",
    "It's recommended to 'unwrap' simulation data to remove potential artifacts from periodic boundary conditions in the simulation.\n",
    "\n",
    "### Method 1 (Better for big systems)\n",
    "The *best* way of doing this requires a trajectory file (i.e. `.xtc`, `.trr`), a `.gro`, and a `.tpr`. By doing this, the input data filesizes can be significantly reduced off the bat by selecting only the lipid groups to keep in the new trajectory file. This can be accomplished by running the following.\n",
    "\n",
    "When prompted to select a group, select only the group of lipids.\n",
    "\n",
    "`$>gmx trjconv -f <trajectory file> -s <.gro file> -o <output trajectory file> -pbc nojump`\n",
    "\n",
    "`$>gmx trjconv -f <.gro file> -s <.tpr file> -o <output .gro file> -pbc nojump`\n",
    "\n",
    "### Method 2\n",
    "This can also be done in one step, with only a trajectory file and a `.gro` file by selecting the group of ALL atoms when prompted.\n",
    "\n",
    "`$>gmx trjconv -f <trajectory file> -s <.gro file> -o <output trajectory file> -pbc nojump`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trajectory_file = \"40nm/run_nojump.xtc\"\n",
    "trr_file = \"40nm/run.trr\"\n",
    "topology_file = \"40nm/system.gro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj as md\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit, fsolve\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection area parameters\n",
    "\n",
    "`spot_radius` is used to determine whether a particle is within the detection area. Currently, this is unused, as cutoff is determined by the sigma of the beam Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Radius of detection area (in nanometers)\n",
    "spot_radius = 10 # Currently unused\n",
    "\n",
    "# Coordinates of detection area center (in nanometers)\n",
    "spotX = 0.0\n",
    "spotY = 0.0\n",
    "spotZ = 10.0 # This isn't used, since we're looking at 2D membranes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   Radial and axial std. dev.s of the Gaussian beam profile\n",
    "w_xy = 20\n",
    "w_z = 2 # Unused\n",
    "k = w_z/w_xy # Unused, just considering a 2-D membrane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various simulation parameters\n",
    "\n",
    "`STEP` is the stride used when iterating through data frames. Data frames are taken every timestep.\n",
    "\n",
    "`INTENSITY` is a scaling constant used to determine the maximum intensity of fluorescence.\n",
    "\n",
    "`SAMPLING_RATIO` defines the percentage of particles to be 'tagged'. Untagged particles are discarded at the beginning of the simulation.\n",
    "\n",
    "`CUTOFF` defines a cutoff for the beam profile. This may be useful to avoid artifacts from periodic boundary conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step size for iterating through data frames\n",
    "STEP = 1\n",
    "\n",
    "# Scaling constant for the intensity of a fluorescing particle\n",
    "INTENSITY = 1\n",
    "\n",
    "# Percentage of tagged particles\n",
    "SAMPLING_RATIO = .8\n",
    "\n",
    "# How many sigmas out from the beam center to truncate the beam's Gaussian profile at\n",
    "CUTOFF = 30\n",
    "\n",
    "# How many lipids to bin into a single trajectory\n",
    "BIN_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diffusion parameters\n",
    "\n",
    "`D` is the diffusion constant for POPC, ~~using data from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1303347/~~ using values from Andrew's STRD paper$^{[2]}$, in units of nm^2/ns.\n",
    "\n",
    "`tauD` is the expected diffusion time, or the time of the half-max for the autocorrelation curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D = .01 From NIH paper\n",
    "D = .0245\n",
    "D = 8\n",
    "tauD = w_xy**2 / (4*D)\n",
    "print(\"Expected diffusion time is %.2f nanoseconds\" % tauD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected diffusion times for various beam waists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_waists = np.arange(10,100,5) # In nanometers\n",
    "print(\"beam waist (nm)\".ljust(16) + \"|\" + \"tau_D (ns)\".rjust(15))\n",
    "print(\"-\"*27)\n",
    "for v in beam_waists:\n",
    "    print(str(v).ljust(16) + \"|\" + str(v ** 2 / (4*D)).rjust(15) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `check_in_detection_volume`\n",
    "Function to check if a given lipid is within the detection volume.\n",
    "\n",
    "Right now, we're interested in contributions from all lipids regardless of position, so the `return True` short circuits it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_in_detection_volume(t, frame_index, residue):\n",
    "\n",
    "    # For now, pay attention to all atoms, regardless of whether or not they're\n",
    "    #   in the detection volume. \n",
    "    # Keep this function as a placeholder, in case this changes.\n",
    "    return True\n",
    "\n",
    "    x, y, z = t.xyz[frame_index, residue._atoms[0].index]\n",
    "\n",
    "    # Get magnitude of distance to the spot center\n",
    "    distance = (x - spotX)**2 + (y - spotY)**2\n",
    "\n",
    "    # Check if the distance is within the spot radius\n",
    "    in_detection_area = distance <= spot_radius**2\n",
    "\n",
    "    return in_detection_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `generate_detection`\n",
    "Defines what happens when a detection is generated from a lipid. Right now, `INTENSITY` is weighted by a 2-D Gaussian determined by the cell's position:\n",
    "\n",
    "$I = I_0 \\exp{ \\left( - \\frac{(x-x_0)^2 + (y-y_0)^2 }{2 \\sigma^2} \\right) }$\n",
    "\n",
    "#### Inputs\n",
    "- `t`: mdtraj.Trajectory object\n",
    "- `frame_index`: index of frame to analyze in trajectory  \n",
    "- `atom`: mdtraj.Atom object to be used for detection\n",
    "\n",
    "#### Returns\n",
    "- Intensity contribution from `atom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_detection(t, frame_index, atom):\n",
    "\n",
    "    # Get coordinates of residue (more correctly, of the P atom)\n",
    "    #x, y, z = t.xyz[frame_index, residue._atoms[0].index]\n",
    "    x, y, z = t.xyz[frame_index, atom.index]\n",
    "    \n",
    "    # Get magnitude of distance to the spot center\n",
    "    distance = (x - spotX)**2 + (y - spotY)**2\n",
    "    \n",
    "    # Truncate at 2 sigma\n",
    "    if distance > CUTOFF**2 * w_xy**2:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate contribution to intensity from an atom, based on the Gaussian\n",
    "    #   profile of the incident beam and the particle's position.\n",
    "    intensity = \\\n",
    "        INTENSITY * np.exp(\n",
    "        -( distance ) # + ((z - spotZ)/k)**2)\n",
    "        / (2 * w_xy**2) )\n",
    "\n",
    "    return intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `analyze_frame`\n",
    "\n",
    "Analyzes the positions of atoms in a given frame, and updates the `detections` list with each atom's position.\n",
    "\n",
    "#### Inputs\n",
    "- `t`: mdtraj.Trajectory object\n",
    "- `frame_index`: index of frame to analyze in trajectory  \n",
    "- `detections`: shared list of all detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_frame(t, frame_index, detections): \n",
    "    \n",
    "    print(\"\\rProcessing frame %d out of %d\" % (frame_index/STEP, len(t)/STEP), end=\"\\r\")\n",
    "\n",
    "    # Iterate through each atom remaining in the topology\n",
    "    for atom in t.topology.atoms:\n",
    "\n",
    "        # Do analysis if atom is in detection volume\n",
    "        if not check_in_detection_volume(t, frame_index, atom):\n",
    "            print(\"Not in detection volume, skipping.\")\n",
    "            continue\n",
    "\n",
    "        detected = generate_detection(t, frame_index, atom)\n",
    "        \n",
    "        detections[atom.index][int(frame_index/STEP)] = detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autocorr_model`\n",
    "\n",
    "Models the expected autocorrelation curve at a given time, for a given diffusion constant.\n",
    "\n",
    "#### Inputs\n",
    "- `t`: Time to calculate autocorrelation at\n",
    "- `D`: Diffusion constant\n",
    "\n",
    "#### Returns\n",
    "- Value of autocorrelation curve at time `t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autocorr_model(t, D):\n",
    "    tauD = w_xy**2 / (4*D) * 2.5\n",
    "    \n",
    "    return (1 + t / tauD)**(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autocorrelate`\n",
    "Computes the autocorrelation curve for a set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autocorrelate(_data, normed=True):\n",
    "    if normed:\n",
    "        normalized = _data/np.linalg.norm(_data)\n",
    "        \n",
    "    autocorrelated = np.correlate(normalized, normalized, mode='full')\n",
    "    autocorrelated = autocorrelated[(autocorrelated.size-1)//2 :]\n",
    "    \n",
    "    return autocorrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trajectory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import FCS data from .xtc file. Also specify a .gro file for the system topology.\n",
    "\n",
    "**NB:** A .trr file can be used for better resolution, since an .xtc typically uses some compression. However, a .trr is also much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#t = md.load(trajectory_file, top=topology_file)\n",
    "#t = md.load_trr(trr_file, top=topology_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate timestep for data analysis, given the simulation data timestep and current stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Timestep for data analysis is %.2f picoseconds (%.2f nanoseconds)\" % (t.timestep * STEP, t.timestep * STEP / 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce atom selection to only phosphorous atoms\n",
    "\n",
    "This is a bit of a simplification, but significantly reduces the amount of atoms to iterate over if we're only considering the phosphorous at the center of the phosphate group. Error from this would be on the order of the bond lengths, so roughly 1.5 angstrom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Starting with %d atoms\" % t.topology.n_atoms)\n",
    "\n",
    "phosphorous_atoms = [a.index for a in t.topology.atoms if a.element.symbol == 'P']\n",
    "t.atom_slice(phosphorous_atoms, inplace=True)\n",
    "\n",
    "print(\"Reduced to %d phosphorous atoms\" % t.topology.n_atoms)\n",
    "\n",
    "# Reduce to the sampling ratio * number of phosphorous atoms\n",
    "num_sampled = int(t.topology.n_atoms * SAMPLING_RATIO)\n",
    "\n",
    "# Randomly select the sampled atoms\n",
    "sampled = np.random.choice([a.index for a in t.topology.atoms], num_sampled, replace=False)\n",
    "t.atom_slice(sampled, inplace=True)\n",
    "\n",
    "print(\"Reduced to %d \\\"tagged\\\" phosphorous atoms\" % t.topology.n_atoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Atom:\n",
    "    \n",
    "    def setIndex(self, i):\n",
    "        self.index = i\n",
    "        \n",
    "class Topology:\n",
    "    \n",
    "    def __init__(self, residues):\n",
    "        self.n_residues = residues\n",
    "        \n",
    "class FakeTrajectory:\n",
    "    \n",
    "    def initialize(self, coords, walkers):\n",
    "        self.topology = Topology(walkers)\n",
    "\n",
    "        self.topology.atoms = list([Atom() for i in range(walkers)])\n",
    "        for i in range(walkers):\n",
    "            self.topology.atoms[i].setIndex(i)\n",
    "\n",
    "        self.topology.n_residues = walkers\n",
    "\n",
    "        self.xyz = coords\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.xyz)\n",
    "        \n",
    "import pickle\n",
    "t = pickle.load(open('../../windrive/linux/output.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of lists to store detected intensity at each timestep for each lipid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "detections = np.full(shape=(t.topology.n_residues, int(np.ceil(len(t)/STEP))), fill_value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Iterate through each frame of data in the trajectory file, and generate a detected intensity from each lipid (represented by its head group P atom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_index in range(0, len(t), STEP):\n",
    "    analyze_frame(t, frame_index, detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin lipids in data\n",
    "\n",
    "`binned_tots[bin][timestep]` is the total intensity for a certain bin at a certain timestep\n",
    "\n",
    "`binned_avgs[bin]` is the average intensity for a bin, over the whole time\n",
    "\n",
    "`binned_dI[bin][timestep]` is the difference of the total intensity from the average at a given timestep\n",
    "\n",
    "`binned_tot[timestep]` is the total intensity from all bins at a certain timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = int(np.ceil(t.topology.n_residues/BIN_SIZE))\n",
    "\n",
    "if not t.topology.n_residues%BIN_SIZE == 0:\n",
    "    print(\"Number of residues is not evenly divisible by bin size. Desired size is %d, one bin will contain %d.\" % (BIN_SIZE, t.topology.n_residues%BIN_SIZE) )\n",
    "\n",
    "print(\"Sorting data into %d groups\" % n_bins)\n",
    "\n",
    "binned =  [ [] for x in range(t.topology.n_residues//BIN_SIZE) ]\n",
    "\n",
    "binned_tots, binned_avgs, binned_dI = [], [], []\n",
    "\n",
    "# TODO: May want to use np.random.choice to randomly select the binned lipids, though the choice of lipids to sample is already random\n",
    "# For each group...\n",
    "for g in range(n_bins):\n",
    "    \n",
    "    # Pick the slice of detections that are relevant to it\n",
    "    _detections = [x for x in detections[g*BIN_SIZE:BIN_SIZE*(g+1)]]\n",
    "        \n",
    "    \n",
    "    avg_I = np.mean(_detections)\n",
    "    tot_I = np.sum(_detections, axis=0)\n",
    "    \n",
    "    delta_I = [tot_I[x] - avg_I for x in range(len(tot_I))]\n",
    "    \n",
    "    \n",
    "    binned_tots.append(tot_I)\n",
    "    binned_avgs.append(avg_I)\n",
    "    binned_dI.append(delta_I)\n",
    "    \n",
    "binned_tot = np.sum(binned_tots, axis=0) - np.mean(binned_tots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity Traces\n",
    "Plot the intensity traces for the individual lipids, and for the summed intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Plot intensity traces #############################\n",
    "# Plot the intensities of detections associated with each lipid\n",
    "plt.subplot(221)\n",
    "for _data in binned_dI:\n",
    "    plt.plot(np.arange(0, len(t), 1), _data, marker='o', markersize=.5 ) \n",
    "plt.xlabel(\"Time (ns)\")\n",
    "plt.ylabel(\"Intensity\")\n",
    "plt.title(\"Intensity Trace\")\n",
    "\n",
    "# Plot sum of intensities (i.e. what a detector would see)\n",
    "plt.subplot(222)\n",
    "plt.plot(np.arange(0, len(t), 1), binned_tot, marker='None', markersize=.5 ) \n",
    "# summed_data = np.sum(intensities, axis=0)\n",
    "# #plt.plot(np.arange(0,len(summed_data), 1), summed_data, marker = 'o', markersize=.1, )\n",
    "# plt.plot(np.arange(0,len(binned), 1), [x[1] for x in binned], marker = 'o', markersize=.1, )\n",
    "plt.xlabel(\"Time (ns)\")\n",
    "plt.ylabel(\"Intensity\")\n",
    "plt.title(\"Summed Intensity Trace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelations\n",
    "Plot the autocorrelation functions for the individual lipids tracked, and for the summed intensities of all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't set the max lag to greater than the number of datapoints..\n",
    "# max_lag = min([100 * int(tauD), (len(t) -1) * STEP])\n",
    "max_lag = (len(t)-1) * STEP\n",
    "\n",
    "tauD = w_xy**2 / (4*D)\n",
    "\n",
    "# Individual data\n",
    "plt.subplot(221)\n",
    "plt.xscale('log')\n",
    "\n",
    "for _data in binned_dI:\n",
    "    plt.acorr(_data, maxlags=max_lag, usevlines=False, linestyle='-', marker=\"None\", normed=True)\n",
    "\n",
    "plt.xlabel(\"Time lag (ns)\")\n",
    "plt.title(\"Normalized Autocorrelation\")\n",
    "\n",
    "# Plot model curve\n",
    "_x = np.arange(0,len(t), 1)\n",
    "_G =(1 + _x / tauD)**(-1)\n",
    "plt.plot(_x, _G, linestyle='--', linewidth=4)\n",
    "    \n",
    "# Plot tau_D, diffusion time\n",
    "plt.axvline(tauD)\n",
    "plt.xticks(list(plt.xticks()[0]) + [tauD], list(plt.xticks()[0]) + ['tau_D'])\n",
    "plt.xlim([10,max_lag])\n",
    "\n",
    "\n",
    "# Summed data\n",
    "plt.subplot(222)\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.acorr(binned_tot, maxlags=max_lag, usevlines=False, linestyle='-', marker=\"None\", normed=True)\n",
    "\n",
    "\n",
    "# Plot model curve\n",
    "_x = np.arange(0,len(t), 1)\n",
    "_G = (1 + _x / tauD)**(-1)\n",
    "plt.plot(_x, _G, linestyle='--', linewidth=4)\n",
    "\n",
    "plt.xlabel(\"Time lag (ns)\")\n",
    "plt.title(\"Normalized Autocorrelation\")\n",
    "    \n",
    "# Plot tau_D, diffusion time\n",
    "plt.axvline(tauD)\n",
    "plt.xticks(list(plt.xticks()[0]) + [tauD], list(plt.xticks()[0]) + ['tau_D'])\n",
    "plt.xlim([0,max_lag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine diffusion coefficient from FCS data\n",
    "\n",
    "The following cells determine the diffusion coefficient from the FCS data using two different techniques. The binned and summed data are treated separately. First, a curve is fit to the autocorrelation curve using the diffusion constant as the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curve Fit Method\n",
    "\n",
    "Using the equation for diffusion in a membrane presented by Schwille$^{[1]}$, attempt to fit the FCS data to a function of the form\n",
    "\n",
    "$G(t) = \\frac{1}{N} \\left(1 + \\frac{t}{\\tau_D} \\right)^{-1}$, where\n",
    "\n",
    "$\\tau_D = \\frac{w_{xy}^2}{4 D}$\n",
    "\n",
    "Since the autocorrelation curves are normalized, $N$ is set to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = np.arange(0, len(t), 1)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.xscale('log')\n",
    "optimals = []\n",
    "covariances = []\n",
    "for _data in binned_dI:\n",
    "    \n",
    "    autocorrelated = autocorrelate(_data)\n",
    "    \n",
    "    _optimal, _covariance = curve_fit(autocorr_model, _x, autocorrelated, p0=D)\n",
    "    optimals.append(_optimal)\n",
    "    covariances.append(_covariance)\n",
    "    \n",
    "    plt.plot(_x, autocorrelated)\n",
    "    plt.plot(_x, autocorr_model(_x, _optimal), linestyle='--')\n",
    "    \n",
    "var = [np.sqrt(np.diag(x)) for x in covariances]\n",
    "    \n",
    "print(\"Average D from data was %f +- %f\" % (np.mean(optimals), np.mean(var)))\n",
    "plt.xlim([0,len(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(222)\n",
    "\n",
    "\n",
    "autocorrelated = autocorrelate(binned_tot)\n",
    "\n",
    "normalized = binned_tot/np.linalg.norm(binned_tot)\n",
    "\n",
    "optimal, covariance = curve_fit(autocorr_model, _x, autocorrelated, p0=D)\n",
    "\n",
    "print(\"D was set at %f\" % D)\n",
    "print(\"D estimated at %f +- %f.\" % (optimal, np.sqrt(np.diag(covariance))))\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.plot(_x, autocorr_model(_x, D), linestyle='--')\n",
    "plt.acorr(binned_tot, maxlags=max_lag, usevlines=False, linestyle='-', marker=\"None\", normed=True)\n",
    "\n",
    "plt.plot(_x, autocorr_model(_x, optimal), linestyle='--', color='black')\n",
    "var = np.sqrt(np.diag(covariance))\n",
    "\n",
    "_y1 = autocorr_model(_x, optimal+var)\n",
    "_y2 = autocorr_model(_x, optimal-var)\n",
    "\n",
    "plt.fill_between(_x, _y1, _y2)\n",
    "\n",
    "plt.xlim([0,len(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .5 crossing method\n",
    "This method determines $\\tau_D$ first by looking for where the normalized autocorrelation crosses 0.5, then computes the diffusion coefficient from that using the same formula as above, solved for $D$\n",
    "\n",
    "$D = \\frac{w_{xy}^2}{4 \\tau_D}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = np.arange(0, len(t), 1)\n",
    "\n",
    "crossings = []\n",
    "\n",
    "guess = .01 # 500 is a magic number, the initial guess for the interpolation. Not great, but it works. Doesn't affect the output.\n",
    "\n",
    "# plt.xscale('log')\n",
    "for _data in binned_dI:\n",
    "    \n",
    "    autocorrelated = autocorrelate(_data)\n",
    "    \n",
    "    interpolated = interp1d(_x, autocorrelated - .5)\n",
    "    crossing = fsolve(interpolated,guess)\n",
    "    crossings.append(crossing)\n",
    "    \n",
    "#     plt.plot(_x, autocorrelated)\n",
    "    \n",
    "\n",
    "calced_D = [w_xy**2 / (4 * x) for x in crossings]\n",
    "    \n",
    "\n",
    "# print(_D)\n",
    "print(\"Average tau_D is %f +- %f\" % (np.mean(crossings), np.std(crossings)))\n",
    "print(\"Average diffusion constant is %f +- %f\" % (np.mean(calced_D), np.std(calced_D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = np.arange(0, len(t), 1)\n",
    "    \n",
    "autocorrelated = autocorrelate(binned_tot)\n",
    "\n",
    "interpolated = interp1d(_x, autocorrelated - .5)\n",
    "crossings = fsolve(interpolated, 1)\n",
    "\n",
    "calced_D = [w_xy**2 / (4 * x) for x in crossings]\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.plot(_x, autocorrelated)\n",
    "plt.plot(_x, autocorr_model(_x, calced_D[0]))\n",
    "\n",
    "print(\"Average tau_D is %f +- %f\" % (np.mean(crossings), np.std(crossings)))\n",
    "print(\"Diffusion constant is %f\" % (np.mean(calced_D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Chiantia, Salvatore, Jonas Ries, and Petra Schwille. \"Fluorescence correlation spectroscopy in membrane structure elucidation.\" Biochimica et Biophysica Acta (BBA)-Biomembranes 1788.1 (2009): 225-233.\n",
    "\n",
    "[2] Zgorski, Andrew, and Edward Lyman. \"Toward Hydrodynamics with Solvent Free Lipid Models: STRD Martini.\" Biophysical journal 111.12 (2016): 2689-2697."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}