{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.252824Z",
     "start_time": "2017-07-31T17:51:39.246212Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual FCS, using MD Simulation Data\n",
    "\n",
    "This goal of this notebook is to generate a simulated FCS measurement using data from a GROMACS simulation.\n",
    "\n",
    "The setup of the virtual system is a membrane with a circularly symmetric incident beam.\n",
    "\n",
    "Data is read in from an .xtc or .trr file, using a .gro file to define the system topology. Data frames from the input file are iterated through, and for each frame, a detected intensity from each lipid is calculated. The intensity trace and autocorrelation functions for each lipid, and for the total at each frame, are plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \u2612 Why don't autocorrelation curves look right? Flatness in middle is unexpected\n",
    "- Done. See below\n",
    " \n",
    "#### \u2612 Generate and plot autocorrelation curve prediction. I'm still not understanding something about autocorrelation curves - I don't see where the sigmoid shape comes from.\n",
    " - Done. Curves need to be plotted on a semilog scale in order to see the expected sigmoid shape.\n",
    " \n",
    "#### \u2612 Try autocorrelating the data myself instead of using acorr. Maybe something weird is happening with the way matplotlib does autocorrelation that isn't desirable.\n",
    " - Done. The manually autocorrelated data is identical to calling plt.acorr with normed=True\n",
    " \n",
    "#### \u2612 Parallelize data analysis\n",
    " - Tried it. The analysis is quite computationally cheap compared to the extra overhead from spawning new threads -- not worth the time saved by running the analysis in parallel.\n",
    " \n",
    "#### \u2612 Handle breaking data up into bins better -- currently ignores the remainder of lipids that don't fall into a bin. (I.e. 11 lipids, bin size 3, the remaining 2 that don't evenly fall into bins are discarded.)\n",
    " - Done. The leftover lipids are put into their own bin, which may be smaller than the BIN_SIZE.\n",
    " \n",
    "#### \u2612 How do PBC vs unwrapped affect prediction of diffusion constant?\n",
    " - Done. Don't appear to significant affect results.\n",
    " \n",
    "#### \u2610 Curve fit method seems to work better for small spot sizes, but 0.5-crossing method seems better for larger spots? Why?\n",
    "\n",
    "#### \u2610 Come up with a way to score the different tests in the Excel spreadsheet. Factor in how close the average is, and how big the error is.\n",
    "     \n",
    "[Checkbox symbols]:<> (\u2612 \u2610)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's recommended to 'unwrap' simulation data to remove potential artifacts from periodic boundary conditions in the simulation.\n",
    "\n",
    "### Method 1 (Better for big systems)\n",
    "The *best* way of doing this requires a trajectory file (i.e. `.xtc`, `.trr`), a `.gro`, and a `.tpr`. By doing this, the input data filesizes can be significantly reduced off the bat by selecting only the lipid groups to keep in the new trajectory file. This can be accomplished by running the following.\n",
    "\n",
    "When prompted to select a group, select only the group of lipids.\n",
    "\n",
    "`$>gmx trjconv -f <trajectory file> -s <.gro file> -o <output trajectory file> -pbc nojump`\n",
    "\n",
    "`$>gmx trjconv -f <.gro file> -s <.tpr file> -o <output .gro file> -pbc nojump`\n",
    "\n",
    "### Method 2\n",
    "This can also be done in one step, with only a trajectory file and a `.gro` file by selecting the group of ALL atoms when prompted.\n",
    "\n",
    "`$>gmx trjconv -f <trajectory file> -s <.gro file> -o <output trajectory file> -pbc nojump`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Notes on input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.262411Z",
     "start_time": "2017-07-31T17:51:39.258012Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trajectory_file = \"40nm/run_nojump.xtc\"\n",
    "trr_file = \"40nm/run.trr\"\n",
    "topology_file = \"40nm/system.gro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.283481Z",
     "start_time": "2017-07-31T17:51:39.268942Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import mdtraj as md\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit, fsolve\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from threading import Lock # For print statements\n",
    "\n",
    "import trajectory\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Detection area parameters\n",
    "\n",
    "`spot_radius` is used to determine whether a particle is within the detection area. Currently, this is unused, as cutoff is determined by the sigma of the beam Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.291122Z",
     "start_time": "2017-07-31T17:51:39.285688Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Radius of detection area (in nanometers)\n",
    "spot_radius = 50 # Currently unused\n",
    "\n",
    "# Coordinates of detection area center (in nanometers)\n",
    "spotX = 0.0\n",
    "spotY = 0.0\n",
    "spotZ = 10.0 # This isn't used, since we're looking at 2D membranes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian parameters\n",
    "\n",
    "Note that what's being set here is the std. deviation `w_xy` of the Gaussian - spot size is probably more accurately represented by the FWHM, or `w_xy * 2.3548`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.312348Z",
     "start_time": "2017-07-31T17:51:39.299538Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   Radial and axial std. dev.s of the Gaussian beam profile\n",
    "FWHM = 250.\n",
    "w_xy = FWHM / (2 * np.sqrt(2 * np.log(2)))\n",
    "w_z = 2 # Unused\n",
    "k = w_z/w_xy # Unused, just considering a 2-D membrane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various simulation parameters\n",
    "\n",
    "`STEP` is the stride used when iterating through data frames. Data frames are taken every timestep.\n",
    "\n",
    "`INTENSITY` is a scaling constant used to determine the maximum intensity of fluorescence.\n",
    "\n",
    "`SAMPLING_RATIO` defines the percentage of particles to be 'tagged'. Untagged particles are discarded at the beginning of the simulation.\n",
    "\n",
    "`CUTOFF` defines a cutoff for the beam profile. This may be useful to avoid artifacts from periodic boundary conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.327994Z",
     "start_time": "2017-07-31T17:51:39.314772Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Step size for iterating through data frames\n",
    "STEP = 1\n",
    "\n",
    "# Scaling constant for the intensity of a fluorescing particle\n",
    "INTENSITY = 1\n",
    "\n",
    "# Percentage of tagged particles\n",
    "SAMPLING_RATIO = 1\n",
    "\n",
    "# How many sigmas out from the beam center to truncate the beam's Gaussian profile at\n",
    "CUTOFF = 2.5\n",
    "\n",
    "# How many lipids to bin into a single trajectory\n",
    "BIN_SIZE = 1\n",
    "\n",
    "# How many random spots to use\n",
    "N_SPOTS = 1\n",
    "\n",
    "# Radius within which to randomly place spots\n",
    "SPOT_RANGE = 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diffusion parameters\n",
    "\n",
    "`D` is the diffusion constant for POPC, ~~using data from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1303347/~~ using values from Andrew's STRD paper$^{[2]}$, in units of nm^2/ns.\n",
    "\n",
    "`tauD` is the expected diffusion time, or the time of the half-max for the autocorrelation curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.336583Z",
     "start_time": "2017-07-31T17:51:39.330049Z"
    }
   },
   "outputs": [],
   "source": [
    "#D = .01 From NIH paper\n",
    "D = .0245\n",
    "D = 4\n",
    "tauD = FWHM**2 / (4*D)\n",
    "print(\"Expected diffusion time is %.2f nanoseconds\" % tauD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Expected diffusion times for various beam waists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.356765Z",
     "start_time": "2017-07-31T17:51:39.346730Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "beam_waists = np.arange(10,100,5) # In nanometers\n",
    "print(\"beam FWHM (nm)\".ljust(16) + \"|\" + \"tau_D (ns)\".rjust(15))\n",
    "print(\"-\"*27)\n",
    "for v in beam_waists:\n",
    "    print(str(v).ljust(16) + \"|\" + str(v ** 2 / (4*D)).rjust(15) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `check_in_detection_volume`\n",
    "Function to check if a given lipid is within the detection volume.\n",
    "\n",
    "Right now, we're interested in contributions from all lipids regardless of position, so the `return True` short circuits it. \n",
    "\n",
    "Furthermore, this is somewhat deprecated by implementing the Gaussian `CUTOFF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.381413Z",
     "start_time": "2017-07-31T17:51:39.368284Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_in_detection_volume(t, frame_index, residue):\n",
    "\n",
    "    # For now, pay attention to all atoms, regardless of whether or not they're\n",
    "    #   in the detection volume. \n",
    "    # Keep this function as a placeholder, in case this changes.\n",
    "    return True\n",
    "\n",
    "    x, y, z = t.xyz[frame_index, residue._atoms[0].index]\n",
    "\n",
    "    # Get magnitude of distance to the spot center\n",
    "    distance = (x - spotX)**2 + (y - spotY)**2\n",
    "\n",
    "    # Check if the distance is within the spot radius\n",
    "    in_detection_area = distance <= spot_radius**2\n",
    "\n",
    "    return in_detection_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `generate_detection`\n",
    "Defines what happens when a detection is generated from a lipid. Right now, `INTENSITY` is weighted by a 2-D Gaussian determined by the cell's position:\n",
    "\n",
    "$I = I_0 \\exp{ \\left( - \\frac{(x-x_0)^2 + (y-y_0)^2 }{2 \\sigma^2} \\right) }$\n",
    "\n",
    "#### Inputs\n",
    "- `t`: mdtraj.Trajectory object\n",
    "- `frame_index`: index of frame to analyze in trajectory  \n",
    "- `atom`: mdtraj.Atom object to be used for detection\n",
    "\n",
    "#### Returns\n",
    "- Intensity contribution from `atom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.415456Z",
     "start_time": "2017-07-31T17:51:39.386728Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_detection(t, frame_index, atom, spotcenter):\n",
    "    \n",
    "#     print(np.shape(t.xyz))\n",
    "    _x = spotcenter[0]\n",
    "    _y = spotcenter[1]\n",
    "\n",
    "    # Get coordinates of residue (more correctly, of the P atom)\n",
    "    #x, y, z = t.xyz[frame_index, residue._atoms[0].index]\n",
    "    try:\n",
    "        x, y, z = t.xyz[frame_index, atom.index]\n",
    "    except IndexError:\n",
    "        raise IndexError(\"Indexing error. Attempting to use frame index %d and atom index %d. Shape is %s\" % (frame_index, atom.index, t.xyz.shape))\n",
    "    \n",
    "    # Get magnitude of distance to the spot center\n",
    "    distance = (x - _x)**2 + (y - _y)**2\n",
    "    \n",
    "    # Truncate at 2 sigma\n",
    "    if distance > CUTOFF**2 * w_xy**2:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate contribution to intensity from an atom, based on the Gaussian\n",
    "    #   profile of the incident beam and the particle's position.\n",
    "    intensity = \\\n",
    "        INTENSITY * np.exp(\n",
    "        -( distance ) # + ((z - spotZ)/k)**2)\n",
    "        / (2 * w_xy**2) )\n",
    "\n",
    "    return intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `analyze_frame`\n",
    "\n",
    "Analyzes the positions of atoms in a given frame, and updates the `detections` list with each atom's position.\n",
    "\n",
    "#### Inputs\n",
    "- `t`: mdtraj.Trajectory object\n",
    "- `frame_index`: index of frame to analyze in trajectory  \n",
    "- `detections`: shared list of all detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.432801Z",
     "start_time": "2017-07-31T17:51:39.417198Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_frame(t, frame_index, detections, spot_number=0, spotcenter=(spotX, spotY)): \n",
    "    \n",
    "    print(\"\\rProcessing frame %d out of %d     \" % (frame_index/STEP, len(t)/STEP), end=\"\\r\")\n",
    "\n",
    "    # Iterate through each atom remaining in the topology\n",
    "    for atom in t.topology.atoms:\n",
    "\n",
    "        # Do analysis if atom is in detection volume\n",
    "        if not check_in_detection_volume(t, frame_index, atom):\n",
    "            print(\"Not in detection volume, skipping.\")\n",
    "            continue\n",
    "\n",
    "        detected = generate_detection(t, frame_index, atom, spotcenter)\n",
    "        \n",
    "        detections[spot_number][atom.index][int(frame_index/STEP)] += detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.446115Z",
     "start_time": "2017-07-31T17:51:39.434597Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_frame_mp(t, frame_index, detections, spotcenter=(spotX, spotY)): \n",
    "    \n",
    "#     print(\"\\rProcessing frame %d out of %d     \" % (frame_index/STEP, len(t)/STEP), end=\"\\r\")\n",
    "\n",
    "    # Iterate through each atom remaining in the topology\n",
    "    for atom in t.topology.atoms:\n",
    "\n",
    "        # Do analysis if atom is in detection volume\n",
    "        if not check_in_detection_volume(t, frame_index, atom):\n",
    "#             print(\"Not in detection volume, skipping.\")\n",
    "            continue\n",
    "\n",
    "        detected = generate_detection(t, frame_index, atom, spotcenter)\n",
    "        \n",
    "#         print(atom.index)\n",
    "        detections[atom.index][int(frame_index/STEP)] += detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autocorr_model`\n",
    "\n",
    "Models the expected autocorrelation curve at a given time, for a given diffusion constant.\n",
    "\n",
    "#### Inputs\n",
    "- `t`: Time to calculate autocorrelation at\n",
    "- `D`: Diffusion constant\n",
    "\n",
    "#### Returns\n",
    "- Value of autocorrelation curve at time `t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.453590Z",
     "start_time": "2017-07-31T17:51:39.447977Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autocorr_model(t, D):\n",
    "    tauD = FWHM**2 / (4*D)\n",
    "    \n",
    "    return (1 + t / tauD)**(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autocorrelate`\n",
    "Computes the autocorrelation curve for a set of data.\n",
    "\n",
    "#### Inputs\n",
    "- `_data`: 1-D array of data\n",
    "- `normed`: Boolean, determines whether to normalize data to unity. Default - `True`\n",
    "\n",
    "#### Returns\n",
    "- `autocorrelated`: Autocorrelation for input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.463331Z",
     "start_time": "2017-07-31T17:51:39.456459Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autocorrelate(_data, normed=True):\n",
    "    if normed:\n",
    "        normalized = _data/np.linalg.norm(_data)\n",
    "        \n",
    "    autocorrelated = np.correlate(normalized, normalized, mode='full')\n",
    "    autocorrelated = autocorrelated[(autocorrelated.size-1)//2 :]\n",
    "    \n",
    "    return autocorrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gauss2d`\n",
    "\n",
    "Computes the value of a 2-D Gaussian.\n",
    "\n",
    "#### Inputs\n",
    "- `xy`: list of (x,y) tuples\n",
    "- `spot`: (x,y) tuple of coordinates of center of detection area\n",
    "- `sigma`: Standard deviation of Gaussian\n",
    "- `cutoff`: Number of standard deviations after which to truncate Gaussian and return 0. Default - `CUTOFF`\n",
    "\n",
    "#### Returns\n",
    "- `z`: A flat list of the value of the Gaussian at each input point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.491595Z",
     "start_time": "2017-07-31T17:51:39.477329Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def gauss2d(xy, spot, sigma, cutoff=CUTOFF):\n",
    "    x, y = xy\n",
    "    x0, y0 = spot\n",
    "    distance = (x - x0)**2 + (y - y0)**2\n",
    "    \n",
    "    # Set values outside the cutoff to 0\n",
    "    s2 = (cutoff * sigma)**2 # Define this so I don't have to keep recomputing it in the list comprehension\n",
    "    distance = np.array([d if d < s2 else 1e10 for d in distance])\n",
    "    \n",
    "    z = np.exp(-(distance)/(2 * sigma**2))\n",
    "    \n",
    "    # Returns an array of the Z values\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_crossings`\n",
    "Gets the 0-crossing of a set of normalized data. This will only find one 0-crossing and can be quite sensitive to the initial guess provided to the solver.\n",
    "\n",
    "#### Inputs\n",
    "- `x`: List of x values\n",
    "- `data`: Normalized set of data\n",
    "- `guess`: Initial guess to solver\n",
    "\n",
    "#### Returns\n",
    "- `crossing`: x coordinate of 0 crossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.510609Z",
     "start_time": "2017-07-31T17:51:39.500817Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_crossing(x, data, guess):\n",
    "    \n",
    "    interpolated = interp1d(x, data - .5, fill_value='extrapolate')\n",
    "#     interpolated = np.interp(x, data - .5, left=1, right=0)\n",
    "    \n",
    "    try:\n",
    "        crossing = fsolve(interpolated, tauD)\n",
    "    except ValueError:\n",
    "        print(\"**** BAD GUESS *****\")\n",
    "        print(\"Skipping this spot...\")\n",
    "        raise ValueError(\"Bad guess.\")\n",
    "        return False\n",
    "        \n",
    "    return crossing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `analyze_spot_mp`\n",
    "\n",
    "Function to analyze detections from a given spot. Suitable for multiprocessed applications.\n",
    "\n",
    "#### Inputs\n",
    "- `t`: Trajectory (or FakeTrajectory) object\n",
    "- `spot`: (x,y) tuple of center of spot to use\n",
    "- `detections`: Queue object to store detections in\n",
    "- `spot_num`: Index of spot.\n",
    "\n",
    "#### Returns\n",
    "- None. However, will add a tuple of (spot_num, \\_detections) to the queue, where detections is an np array of shape (n_residues, number of timesteps) storing the detections for each residue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.531800Z",
     "start_time": "2017-07-31T17:51:39.522053Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_spot(t, spot, detections, spot_num):\n",
    "    for frame_index in range(0, len(t), STEP):\n",
    "        analyze_frame(t, frame_index, detections, spot_num, spot)\n",
    "\n",
    "def analyze_spot_mp(t, spot, detections, spot_num):\n",
    "    _detections = np.full(shape=(t.topology.n_residues, int(np.ceil(len(t)/STEP))), fill_value=0.0) \n",
    "    for frame_index in range(0, len(t), STEP):\n",
    "        analyze_frame_mp(t, frame_index, _detections, spot)\n",
    "    detections.put((spot_num, _detections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load trajectory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Import FCS data from .xtc file. Also specify a .gro file for the system topology.\n",
    "\n",
    "**NB:** A .trr file can be used for better resolution, since an .xtc typically uses some compression. However, a .trr is also much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.555404Z",
     "start_time": "2017-07-31T17:51:39.551207Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#t = md.load(trajectory_file, top=topology_file)\n",
    "#t = md.load_trr(trr_file, top=topology_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Calculate timestep for data analysis, given the simulation data timestep and current stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.577127Z",
     "start_time": "2017-07-31T17:51:39.575040Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(\"Timestep for data analysis is %.2f picoseconds (%.2f nanoseconds)\" % (t.timestep * STEP, t.timestep * STEP / 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Reduce atom selection to only phosphorous atoms\n",
    "\n",
    "This is a bit of a simplification, but significantly reduces the amount of atoms to iterate over if we're only considering the phosphorous at the center of the phosphate group. Error from this would be on the order of the bond lengths, so roughly 1.5 angstrom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:51:39.606944Z",
     "start_time": "2017-07-31T17:51:39.602779Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(\"Starting with %d atoms\" % t.topology.n_atoms)\n",
    "\n",
    "# phosphorous_atoms = [a.index for a in t.topology.atoms if a.element.symbol == 'P']\n",
    "# t.atom_slice(phosphorous_atoms, inplace=True)\n",
    "\n",
    "# print(\"Reduced to %d phosphorous atoms\" % t.topology.n_atoms)\n",
    "\n",
    "# # Reduce to the sampling ratio * number of phosphorous atoms\n",
    "# num_sampled = int(t.topology.n_atoms * SAMPLING_RATIO)\n",
    "\n",
    "# # Randomly select the sampled atoms\n",
    "# sampled = np.random.choice([a.index for a in t.topology.atoms], num_sampled, replace=False)\n",
    "# t.atom_slice(sampled, inplace=True)\n",
    "\n",
    "# print(\"Reduced to %d \\\"tagged\\\" phosphorous atoms\" % t.topology.n_atoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:52:05.421937Z",
     "start_time": "2017-07-31T17:51:39.627708Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "t = pickle.load(open('../../windrive/linux/output.pkl', 'rb'))\n",
    "print(\"Shape is %s, reducing by %d%%.\" % (np.shape(t.xyz), 100-SAMPLING_RATIO*100))\n",
    "# t.xyz = t.xyz[:10000]\n",
    "t.reduce(SAMPLING_RATIO)\n",
    "print(np.shape(t.xyz))\n",
    "\n",
    "trajectories = []\n",
    "print(int(t.topology.n_residues / BIN_SIZE))\n",
    "\n",
    "# BIN_SIZE is used a little differently with this technique -- applies to binning trajectories in the experiment itself, rather than binning experimental data during analysis.\n",
    "import copy\n",
    "for traj in range(int(t.topology.n_residues / BIN_SIZE)):\n",
    "    _t = trajectory.FakeTrajectory()\n",
    "    _t.initialize(t.xyz[:,BIN_SIZE*traj:BIN_SIZE*(traj+1)], BIN_SIZE)\n",
    "#     _t = copy.deepcopy(t)\n",
    "#     _t.xyz = t.xyz[:,BIN_SIZE*traj:BIN_SIZE*(traj+1)]\n",
    "#     _t.topology.n_residues = BIN_SIZE\n",
    "#     _t.topology.atoms = t.topology.atoms[BIN_SIZE*traj:BIN_SIZE*(traj+1)]\n",
    "#     _t.reindex()\n",
    "    trajectories.append(_t)\n",
    "N_SPOTS = len(trajectories)\n",
    "print(N_SPOTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of lists to store detected intensity at each timestep for each lipid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:52:05.476481Z",
     "start_time": "2017-07-31T17:52:05.423489Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# detections = np.full(shape=(N_SPOTS, t.topology.n_residues, int(np.ceil(len(t)/STEP))), fill_value=0.0)\n",
    "detections = np.full(shape=(len(trajectories), BIN_SIZE, int(np.ceil(len(t)/STEP))), fill_value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Iterate through each frame of data in the trajectory file, and generate a detected intensity from each lipid (represented by its head group P atom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:52:05.481035Z",
     "start_time": "2017-07-31T17:52:05.478049Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spot_centers = np.random.uniform(-SPOT_RANGE, SPOT_RANGE, size=(N_SPOTS,2))\n",
    "spot_center = np.random.uniform(-SPOT_RANGE, SPOT_RANGE, size=(3,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Legacy threaded implementation\n",
    "\n",
    "Much slower than the process equivalent code below. Keeping it *just in case*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This threaded code works on having each analyze_spot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:52:05.486145Z",
     "start_time": "2017-07-31T17:52:05.482271Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# spot_centers = np.random.uniform(-SPOT_RANGE, SPOT_RANGE, size=(N_SPOTS,2))\n",
    "\n",
    "# def analyze_spot(t, spot, detections, spot_num):\n",
    "#     for frame_index in range(0, len(t), STEP):\n",
    "#         analyze_frame(t, frame_index, detections, spot_num, spot)\n",
    "\n",
    "# pool = ThreadPool()\n",
    "\n",
    "# print(\"Performing analysis for %d randomly assigned spots, parallelized over %d cores.\" % (N_SPOTS, multiprocessing.cpu_count()))\n",
    "\n",
    "# for i in range(N_SPOTS):\n",
    "#     pool.apply_async(analyze_spot, args=(t, spot_centers[i], detections, i))\n",
    "    \n",
    "# pool.close()\n",
    "# pool.join()\n",
    "\n",
    "# print(\"\\n\\n\", end='', flush=True) # Flush stdout so the printed output doesn't spread itself into the next few output cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate over a number of random spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:52:05.495143Z",
     "start_time": "2017-07-31T17:52:05.487483Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"Performing analysis for %d randomly assigned spots, parallelized over %d cores.\" % (N_SPOTS, multiprocessing.cpu_count()))\n",
    "\n",
    "# # Set up a pool to draw workers from.\n",
    "# pool = multiprocessing.Pool()\n",
    "\n",
    "# # Need to use a Manager to share the Queue across multiple workers in different processes.\n",
    "# #   Using a Queue allows each process to access a shared object. Typically processes cannot do this\n",
    "# #   but threads can. However, implementing this parallelization in processes yields a ~6x speedup\n",
    "# #   over the threaded equivalent.\n",
    "# m = multiprocessing.Manager()\n",
    "# detections_Q = m.Queue(N_SPOTS)\n",
    "\n",
    "\n",
    "# # Make a list to hold AsyncResult objects. This is useful for debugging - call job.get() for an element to see\n",
    "# #    any exceptions that were raised. Otherwise, they'll fail silently.\n",
    "# jobs = [] \n",
    "# for i in range(N_SPOTS):\n",
    "#     jobs += [pool.apply_async(analyze_spot_mp, args=(t, spot_centers[i], detections_Q, i))]\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "\n",
    "\n",
    "# # Process the queued data, and add it to the detections list.\n",
    "# print(\"Processing queued data\")\n",
    "\n",
    "# # detections = [x for x in iter(detections_Q.get, None)] # This should work, but throws a ValueError. Whatever.\n",
    "# for i in range(N_SPOTS):\n",
    "#     if detections_Q.empty():\n",
    "#         print(\"Queue prematurely empty..\")\n",
    "#         break\n",
    "#     _spot, _detection = detections_Q.get()\n",
    "#     detections[_spot] = _detection\n",
    "    \n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate over entire dataset, vs over multiple random spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:53:11.590607Z",
     "start_time": "2017-07-31T17:52:05.496474Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Performing analysis for %d sets of trajectories, parallelized over %d cores.\" % (len(trajectories), multiprocessing.cpu_count()))\n",
    "\n",
    "# Set up a pool to draw workers from.\n",
    "pool = multiprocessing.Pool()\n",
    "\n",
    "# Need to use a Manager to share the Queue across multiple workers in different processes.\n",
    "#   Using a Queue allows each process to access a shared object. Typically processes cannot do this\n",
    "#   but threads can. However, implementing this parallelization in processes yields a ~6x speedup\n",
    "#   over the threaded equivalent.\n",
    "m = multiprocessing.Manager()\n",
    "detections_Q = m.Queue(len(trajectories))\n",
    "\n",
    "\n",
    "# Make a list to hold AsyncResult objects. This is useful for debugging - call job.get() for an element to see\n",
    "#    any exceptions that were raised. Otherwise, they'll fail silently.\n",
    "jobs = []\n",
    "    \n",
    "# print(np.shape(trajectories[0].xyz))\n",
    "# analyze_spot_mp(trajectories[0], spot_center[0], detections_Q, 0)\n",
    "# Instead of iterating over each spot, iterate over slices of the trajectory\n",
    "for i in range(len(trajectories)):\n",
    "    jobs += [pool.apply_async(analyze_spot_mp, args=(trajectories[i], [0,0], detections_Q, i))]\n",
    "# print([job.get() for job in jobs])\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:53:12.113314Z",
     "start_time": "2017-07-31T17:53:11.592862Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process the queued data, and add it to the detections list.\n",
    "print(\"Processing queued data\")\n",
    "\n",
    "# detections = [x for x in iter(detections_Q.get, None)] # This should work, but throws a ValueError. Whatever.\n",
    "for i in range(len(trajectories)):\n",
    "#     print(\"Analyzing queue entry %d\" % i)\n",
    "    if detections_Q.empty():\n",
    "        print(\"Queue prematurely empty..\")\n",
    "        break\n",
    "    _spot, _detection = detections_Q.get()\n",
    "    detections[_spot] = _detection\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:53:12.117295Z",
     "start_time": "2017-07-31T17:53:12.114412Z"
    }
   },
   "outputs": [],
   "source": [
    "detections.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Bin lipids in data\n",
    "\n",
    "`binned_tots[bin][timestep]` is the total intensity for a certain bin at a certain timestep\n",
    "\n",
    "`binned_avgs[bin]` is the average intensity for a bin, over the whole time\n",
    "\n",
    "`binned_dI[bin][timestep]` is the difference of the total intensity from the average at a given timestep\n",
    "\n",
    "`binned_tot[timestep]` is the average dI from all bins at a certain timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:53:26.088445Z",
     "start_time": "2017-07-31T17:53:12.118308Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_bins = int(np.ceil(trajectories[0].topology.n_residues/BIN_SIZE))\n",
    "\n",
    "print(\"Attempting to bin %d residues into %d bins.\" % (t.topology.n_residues, BIN_SIZE))\n",
    "\n",
    "if not t.topology.n_residues%BIN_SIZE == 0:\n",
    "    print(\"Number of residues is not evenly divisible by bin size. Desired size is %d, one bin will contain %d.\" % (BIN_SIZE, t.topology.n_residues%BIN_SIZE) )\n",
    "\n",
    "binned_tots = np.ndarray(shape=(N_SPOTS, n_bins, len(t)))\n",
    "binned_avgs = np.ndarray(shape=(N_SPOTS, n_bins, len(t)))\n",
    "binned_dI = np.ndarray(shape=(N_SPOTS, n_bins, len(t)))\n",
    "binned_tot = np.ndarray(shape=(N_SPOTS, len(t)))\n",
    "    \n",
    "for spot_num in range(N_SPOTS):\n",
    "#     print(\"Sorting data into %d groups\" % n_bins)\n",
    "\n",
    "    binned =  [ [] for x in range(t.topology.n_residues//BIN_SIZE) ]\n",
    "\n",
    "\n",
    "    # TODO: May want to use np.random.choice to randomly select the binned lipids, though the choice of lipids to sample is already random\n",
    "    # For each group...\n",
    "    for g in range(n_bins):\n",
    "\n",
    "        # Pick the slice of detections that are relevant to it\n",
    "        _detections = [x for x in detections[spot_num][g*BIN_SIZE:BIN_SIZE*(g+1)]]\n",
    "#         print(\"Detections_spot_num length is %d \" % len(detections[spot_num]))\n",
    "#         print(len(_detections))\n",
    "\n",
    "\n",
    "        avg_I = np.mean(_detections)\n",
    "        tot_I = np.sum(_detections, axis=0)\n",
    "\n",
    "        delta_I = [tot_I[x] - avg_I for x in range(len(tot_I))]\n",
    "\n",
    "\n",
    "        binned_tots[spot_num][g] = tot_I\n",
    "        binned_avgs[spot_num][g] = avg_I\n",
    "        binned_dI[spot_num][g] = delta_I\n",
    "\n",
    "    # binned_tot = np.sum(binned_tots, axis=0) - np.mean(binned_tots)\n",
    "    binned_tot[spot_num] = np.mean(binned_dI[spot_num], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute average numbers of particles contributing to the intensity. (I.e., that have a nonzero intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:53:26.092951Z",
     "start_time": "2017-07-31T17:53:26.090042Z"
    }
   },
   "outputs": [],
   "source": [
    "detections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:53:26.154537Z",
     "start_time": "2017-07-31T17:53:26.094090Z"
    }
   },
   "outputs": [],
   "source": [
    "n_nonzero = 0\n",
    "n_tot = 0\n",
    "n_zero = 0\n",
    "\n",
    "# Get the number of particles that don't contribute\n",
    "for i in range(N_SPOTS):\n",
    "    n_zero += np.all(detections[i,:,:] == 0)\n",
    "print(\"%d particles contribute intensities to this spot\" % (t.topology.n_residues - n_zero))\n",
    "\n",
    "\n",
    "# # Get total number of nonzero contributions\n",
    "# for spot_num in range(N_SPOTS):\n",
    "#     for particle in detections[spot_num]:\n",
    "#         _nonzero = [x for x in particle if not x == 0.0 ]\n",
    "#         n_nonzero += len(_nonzero)\n",
    "#         n_tot += len(particle)\n",
    "    \n",
    "# # Get total number of detections\n",
    "# # n_detections = N_SPOTS * t.topology.n_residues * len(t)\n",
    "\n",
    "# print(\"Average percentage of the %d sampled particles that contribute to intensity is %f%%\" % (t.topology.n_residues, 100 * n_nonzero / n_tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity Traces\n",
    "Plot the intensity traces for the individual lipids, and for the summed intensities."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T14:44:52.195904Z",
     "start_time": "2017-07-31T14:42:12.312938Z"
    },
    "collapsed": true
   },
   "source": [
    "################## Plot intensity traces #############################\n",
    "# Plot the intensities of detections associated with each lipid\n",
    "plt.subplot(221)\n",
    "# for _data in binned_dI:\n",
    "#     plt.plot(np.arange(0, len(t), 1), _data, marker='o', markersize=.5 ) \n",
    "plt.xlabel(\"Time (ns)\")\n",
    "plt.ylabel(\"Intensity\")\n",
    "plt.title(\"Intensity Trace\")\n",
    "\n",
    "# Plot sum of intensities (i.e. what a detector would see)\n",
    "plt.subplot(222)\n",
    "for spot_num in range(N_SPOTS):\n",
    "    plt.plot(np.arange(0, len(t), 1), binned_tot[spot_num], marker='None', label=\"Spot %d\" % spot_num)\n",
    "# plt.plot(np.arange(0, len(t), 1), binned_tot[0], marker='None', markersize=.5 ) \n",
    "# summed_data = np.sum(intensities, axis=0)\n",
    "# #plt.plot(np.arange(0,len(summed_data), 1), summed_data, marker = 'o', markersize=.1, )\n",
    "# plt.plot(np.arange(0,len(binned), 1), [x[1] for x in binned], marker = 'o', markersize=.1, )\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time (ns)\")\n",
    "plt.ylabel(\"Intensity\")\n",
    "plt.title(\"Summed Intensity Trace\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:53:26.157814Z",
     "start_time": "2017-07-31T17:53:26.155947Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(0, len(t), 1), binned_tot[11], marker='None', label=\"Spot %d\" % spot_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelations\n",
    "Plot the autocorrelation functions for the individual lipids tracked, and for the summed intensities of all of them."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T15:29:05.065664Z",
     "start_time": "2017-07-31T15:13:00.859316Z"
    }
   },
   "source": [
    "# Don't set the max lag to greater than the number of datapoints..\n",
    "# max_lag = min([100 * int(tauD), (len(t) -1) * STEP])\n",
    "max_lag = (len(t)-1) * STEP\n",
    "\n",
    "tauD = FWHM**2 / (4*D)\n",
    "\n",
    "# # Individual data\n",
    "# plt.subplot(221)\n",
    "# plt.xscale('log')\n",
    "\n",
    "# # for _data in binned_dI:\n",
    "# #     plt.acorr(_data, maxlags=max_lag, usevlines=False, linestyle='-', marker=\"None\", normed=True)\n",
    "\n",
    "# plt.xlabel(\"Time lag (ns)\")\n",
    "# plt.title(\"Normalized Autocorrelation\")\n",
    "\n",
    "# # Plot model curve\n",
    "# _x = np.arange(0,len(t), 1)\n",
    "# _G =(1 + _x / tauD)**(-1)\n",
    "# plt.plot(_x, _G, linestyle='--', linewidth=4)\n",
    "    \n",
    "# # Plot tau_D, diffusion time\n",
    "# plt.axvline(tauD)\n",
    "# plt.xticks(list(plt.xticks()[0]) + [tauD], list(plt.xticks()[0]) + ['tau_D'])\n",
    "# plt.xlim([0,max_lag])\n",
    "\n",
    "\n",
    "# Summed data\n",
    "plt.subplot(222)\n",
    "plt.xscale('log')\n",
    "\n",
    "_x = np.arange(0, len(t), 1)\n",
    "print(\"Beginning analysis\")\n",
    "for spot_num in range(N_SPOTS):\n",
    "    print(\"Analyzing spot %d\" % spot_num, end=\"\\r\")\n",
    "    autocorrelated = autocorrelate(binned_tot[spot_num])\n",
    "    plt.plot(_x, autocorrelated, color=color, label=\"Data - Spot %d\" % spot_num)\n",
    "#     plt.acorr(binned_tot[spot_num], maxlags=max_lag, usevlines=False, linestyle='-', marker=\"None\", normed=True, label=\"Data - Spot %d\" % spot_num)\n",
    "\n",
    "\n",
    "# Plot model curve\n",
    "_x = np.arange(0,len(t), 1)\n",
    "# _G = (1 + _x / tauD)**(-1)\n",
    "plt.plot(_x, autocorr_model(_x, D*1), linestyle='--', color='k', linewidth=4, label=\"Model\")\n",
    "\n",
    "plt.xlabel(\"Time lag (ns)\")\n",
    "plt.title(\"Normalized Autocorrelation\")\n",
    "    \n",
    "# Plot tau_D, diffusion time\n",
    "plt.axvline(tauD)\n",
    "plt.xticks(list(plt.xticks()[0]) + [tauD], list(plt.xticks()[0]) + ['tau_D'])\n",
    "plt.xlim([0,max_lag])\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine diffusion coefficient from FCS data\n",
    "\n",
    "The following cells determine the diffusion coefficient from the FCS data using two different techniques. The binned and summed data are treated separately. First, a curve is fit to the autocorrelation curve using the diffusion constant as the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curve Fit Method\n",
    "\n",
    "Using the equation for diffusion in a membrane presented by Schwille$^{[1]}$, attempt to fit the FCS data to a function of the form\n",
    "\n",
    "$G(t) = \\frac{1}{N} \\left(1 + \\frac{t}{\\tau_D} \\right)^{-1}$, where\n",
    "\n",
    "$\\tau_D = \\frac{w_{xy}^2}{4 D}$\n",
    "\n",
    "Since the autocorrelation curves are normalized, $N$ is set to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Binned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-31T17:53:26.168005Z",
     "start_time": "2017-07-31T17:53:26.158861Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# _x = np.arange(0, len(t), 1)\n",
    "\n",
    "# plt.xscale('log')\n",
    "# optimals = []\n",
    "# covariances = []\n",
    "# i = 0\n",
    "# for _data in binned_dI:\n",
    "#     i+= 1\n",
    "#     print(i, end=\"\\r\")\n",
    "    \n",
    "#     autocorrelated = autocorrelate(_data)\n",
    "    \n",
    "#     _optimal, _covariance = curve_fit(autocorr_model, _x, autocorrelated, p0=D)\n",
    "#     optimals.append(_optimal)\n",
    "#     covariances.append(_covariance)\n",
    "    \n",
    "#     plt.plot(_x, autocorrelated)\n",
    "#     plt.plot(_x, autocorr_model(_x, _optimal), linestyle='--')\n",
    "    \n",
    "# var = [np.sqrt(np.diag(x)) for x in covariances]\n",
    "    \n",
    "# print(\"Average D from data was %f +- %f\" % (np.mean(optimals), np.mean(var)))\n",
    "# plt.xlim([0,len(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Summed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-31T17:51:42.442Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# autocorrelated = autocorrelate(binned_tot)\n",
    "\n",
    "# normalized = binned_tot/np.linalg.norm(binned_tot)\n",
    "\n",
    "# optimal, covariance = curve_fit(autocorr_model, _x, autocorrelated, p0=D)\n",
    "\n",
    "# print(\"D was set at %f\" % D)\n",
    "# print(\"D estimated at %f +- %f.\" % (optimal, np.sqrt(np.diag(covariance))))\n",
    "\n",
    "# plt.xscale('log')\n",
    "\n",
    "# # Orange line is using the known diffusion constant\n",
    "# plt.plot(_x, autocorr_model(_x, D), linestyle='--', label=\"Model - Known D\")\n",
    "# plt.acorr(binned_tot, maxlags=max_lag, usevlines=False, linestyle='-', marker=\"None\", normed=True, label=\"Data\")\n",
    "\n",
    "# # Black line is using the calculated diffusion constant\n",
    "# plt.plot(_x, autocorr_model(_x, optimal), linestyle='--', color='black', label=\"Model - Calced D\")\n",
    "# var = np.sqrt(np.diag(covariance))\n",
    "\n",
    "# _y1 = autocorr_model(_x, optimal+var)\n",
    "# _y2 = autocorr_model(_x, optimal-var)\n",
    "\n",
    "# plt.fill_between(_x, _y1, _y2)\n",
    "\n",
    "# plt.xlim([0,len(t)])\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .5 crossing method\n",
    "This method determines $\\tau_D$ first by looking for where the normalized autocorrelation crosses 0.5, then computes the diffusion coefficient from that using the same formula as above, solved for $D$\n",
    "\n",
    "$D = \\frac{w_{xy}^2}{4 \\tau_D}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-31T17:51:42.479Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_x = np.arange(0, len(t), 1)\n",
    "ax = plt.gca()\n",
    "\n",
    "crossings = []\n",
    "\n",
    "plt.xscale('log')\n",
    "for spot_num in range(N_SPOTS):\n",
    "    for _data in binned_dI[spot_num]:\n",
    "\n",
    "        autocorrelated = autocorrelate(_data)\n",
    "    \n",
    "        try:\n",
    "            crossings.append(get_crossing(_x, autocorrelated, tauD))\n",
    "        except ValueError:\n",
    "            print(\"Bad guess. Skipping a bin in spot %d\" % (spot_num))\n",
    "            continue\n",
    "\n",
    "        color = ax._get_lines.get_next_color()\n",
    "        \n",
    "        plt.plot(_x, autocorrelated, color=color)\n",
    "\n",
    "        calced_D = FWHM**2 / (4 * crossings[-1])\n",
    "        plt.plot(_x, autocorr_model(_x, calced_D), color=color, linestyle='--')\n",
    "    \n",
    "\n",
    "# calced_D = [FWHM**2 / (4 * x) for x in crossings]\n",
    "calced_D = FWHM**2 / (4 * np.mean(crossings))\n",
    "\n",
    "\n",
    "plt.plot(_x, autocorr_model(_x, D), color='k', linestyle='-', linewidth=2, label=\"Model - Expected\")\n",
    "    \n",
    "\n",
    "# Plot tau_D, diffusion time\n",
    "calc_tauD = FWHM**2 / (4 * np.mean(calced_D))\n",
    "plt.axvline(calc_tauD)\n",
    "plt.axvline(tauD)\n",
    "\n",
    "plt.xticks(list(plt.xticks()[0]) + [calc_tauD, tauD], list(plt.xticks()[0]) + ['calculated tau_D', 'expected tau_D'])\n",
    "\n",
    "plt.xlim([0,len(t)])\n",
    "\n",
    "print(\"Average tau_D is %f +- %f\" % (np.mean(crossings), np.std(crossings)))\n",
    "print(\"Average diffusion constant is %.3f +- %.4f\" % (np.mean(calced_D), np.std(calced_D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summed data for each spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-31T17:51:42.522Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.xscale('log')\n",
    "ax = plt.gca()\n",
    "_x = np.arange(0, len(t), 1)\n",
    "\n",
    "Ds = []\n",
    "\n",
    "for spot_num in range(N_SPOTS):\n",
    "    \n",
    "    print(\"--- Spot %d ---\" % spot_num, end=\"\\r\")\n",
    "    \n",
    "    if np.sum(binned_tot[spot_num]) == 0.0:\n",
    "#         print(\"This spot is all 0, skipping\", end=\"\")\n",
    "        continue\n",
    "    \n",
    "    autocorrelated = autocorrelate(binned_tot[spot_num])\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        crossing = get_crossing(_x, autocorrelated, tauD*.01)\n",
    "    except ValueError:\n",
    "        print(\"Bad guess. Skipping a bin in spot %d\" % (spot_num))\n",
    "        continue\n",
    "    \n",
    "#     print(crossing)\n",
    "\n",
    "    calced_D = FWHM**2 / (4 * crossing)\n",
    "\n",
    "    color = ax._get_lines.get_next_color()\n",
    "    plt.plot(_x, autocorrelated, color=color, label=\"Data - Spot %d\" % spot_num)\n",
    "    plt.plot(_x, autocorr_model(_x, calced_D), color=color, linestyle='--', label=\"Model - Spot %d\" % spot_num)\n",
    "\n",
    "    print(\"\\nCalculated D is %.3f, tau_D is %.3f\" %( calced_D, crossing))\n",
    "    \n",
    "    Ds.append(calced_D)\n",
    "\n",
    "\n",
    "plt.plot(_x, autocorr_model(_x, D), color='k', linestyle='--', linewidth=2, label=\"Model - Expected\")\n",
    "    \n",
    "# Plot tau_D, diffusion time\n",
    "\n",
    "calc_tauD = FWHM**2 / (4 * np.mean(Ds))\n",
    "print(\"\\n\\nAvg. D is %.3f +- %.4f, yielding a tau_D of %.2f compared to the expected %.2f for D=%d\" % (np.mean(Ds), np.std(Ds), calc_tauD, tauD, D))\n",
    "\n",
    "plt.axvline(calc_tauD)\n",
    "plt.axvline(tauD)\n",
    "\n",
    "plt.xticks(list(plt.xticks()[0]) + [calc_tauD, tauD], list(plt.xticks()[0]) + ['calculated tau_D', 'expected tau_D'])\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "plt.xlim([0,len(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average autocorrelations for each spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-31T17:51:42.560Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.xscale('log')\n",
    "ax = plt.gca()\n",
    "_x = np.arange(0, len(t), 1)\n",
    "\n",
    "autocorrelated = np.ndarray(shape=(N_SPOTS,len(t)))\n",
    "\n",
    "# Average autocorrelations for each spot\n",
    "for spot_num in range(N_SPOTS):\n",
    "    \n",
    "    if np.sum(binned_tot[spot_num]) == 0.0:\n",
    "#         print(\"Spot %d is all 0, setting to NaN\" % spot_num)\n",
    "        \n",
    "        # If the spot didn't capture any intensity contributions, just set it to NaN.\n",
    "        # NaN values will be ignored when taking the mean. (Thanks, np.nanmean)\n",
    "        autocorrelated[spot_num] = np.full(len(t), np.NAN)\n",
    "        continue\n",
    "    \n",
    "    autocorrelated[spot_num] = autocorrelate(binned_tot[spot_num])\n",
    "    \n",
    "autocorrelated = np.nanmean(autocorrelated, axis=0)\n",
    "\n",
    "crossing = get_crossing(_x, autocorrelated, tauD)\n",
    "\n",
    "calced_D = FWHM**2 / (4 * crossing)\n",
    "\n",
    "print(calced_D)\n",
    "\n",
    "plt.plot(_x, autocorrelated, label=\"Averaged spot data\")\n",
    "\n",
    "plt.plot(_x, autocorr_model(_x, calced_D), linestyle='-', linewidth=1, label=\"Model - Calculated\")\n",
    "\n",
    "plt.plot(_x, autocorr_model(_x, D), color='k', linestyle='--', linewidth=2, label=\"Model - Expected\")\n",
    "\n",
    "plt.axvline(crossing)\n",
    "plt.axvline(tauD)\n",
    "plt.xticks(list(plt.xticks()[0]) + [crossing, tauD], list(plt.xticks()[0]) + ['calculated tau_D', 'expected tau_D'])\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this is to plot the spots overlaid on the trajectories of the sampled particles, with a heatmap indicating the Gaussian profile that's being used to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-31T17:51:42.608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make the aspect ratio of the plot square so that the spot circles are circles and not ellipses\n",
    "plt.axis('equal') \n",
    "\n",
    "############## Plot trajectories ############## \n",
    "for w in range(0, t.topology.n_residues, 1):\n",
    "    plt.plot(t.xyz[:,w,0], t.xyz[:,w,1], linestyle='-', linewidth=.2, zorder=1, alpha=.6)\n",
    "\n",
    "\n",
    "############## Plot spots ############## \n",
    "ax = plt.gca()\n",
    "for spot in spot_centers:\n",
    "    circle = plt.Circle(spot, FWHM, fill=False, linewidth=2, ec='white', zorder=3)\n",
    "    ax.add_artist(circle)\n",
    "    \n",
    "    \n",
    "############## Plot heatmap ############## \n",
    "# Get axis ranges to use for heatmap point grid\n",
    "xrange = ax.get_xlim()\n",
    "yrange = ax.get_ylim()\n",
    "\n",
    "# Set the number of points. Make this imaginary so that np.mgrid includes the endpoints.\n",
    "npoints = 500j\n",
    "zpreds = []\n",
    "\n",
    "# Make a grid of points that will be used to display the Gaussians. May also be able to do this as a countour..\n",
    "yi, xi = np.mgrid[yrange[1]:yrange[0]:npoints, xrange[0]:xrange[1]:npoints]\n",
    "xyi = np.vstack([xi.ravel(), yi.ravel()])\n",
    "\n",
    "# Determine Gaussian profiles for each spot\n",
    "for spot in spot_centers:\n",
    "    zpred = gauss2d(xyi, spot, w_xy, cutoff=CUTOFF)\n",
    "    zpred.shape = xi.shape # Convert Z values from a flat list to a 2-D (x,y) array\n",
    "    zpreds.append(zpred)\n",
    "    plt.annotate(xy=spot, s=\"Spot %d\" % np.where(spot_centers==spot)[0][0], color='w') # The `where` is some magic to get the index of the spot\n",
    "    \n",
    "# Set each Z value to the max at that position, most meaningful way of plotting multiple profiles (?)\n",
    "zpreds = np.maximum.reduce(zpreds)\n",
    "\n",
    "# Display Gaussian profiles\n",
    "im = ax.imshow(zpreds, extent=[xi.min(), xi.max(), yi.min(), yi.max()], aspect='equal', zorder=0, alpha=1, cmap='magma')\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label(\"Gaussian profile - Intensity Contribution\",size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Chiantia, Salvatore, Jonas Ries, and Petra Schwille. \"Fluorescence correlation spectroscopy in membrane structure elucidation.\" Biochimica et Biophysica Acta (BBA)-Biomembranes 1788.1 (2009): 225-233.\n",
    "\n",
    "[2] Zgorski, Andrew, and Edward Lyman. \"Toward Hydrodynamics with Solvent Free Lipid Models: STRD Martini.\" Biophysical journal 111.12 (2016): 2689-2697."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "1016px",
    "left": "0px",
    "right": "2211.5px",
    "top": "106px",
    "width": "333px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}